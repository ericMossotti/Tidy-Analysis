---
title: "Professional Data Analysis"
author: "Eric Mossotti"

code-fold: true

#df_print: "tibble" 

format: html
toc: true
---

```{r include = FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)
```

# Overview

The dataset is immense with millions of observations from just a single year. It seems to be regularly updated with more data, interestingly enough, despite being simulated.

This data doesn't require much domain knowledge to understand, but it is realistic (with errors) and challenging enough to be worthy of processing and analysis.

## Importing Data Efficiently

I think .ZIP files would be commonplace as they substantially reduce long term storage requirements. Here, I wrote a short function that downloads a set of zip files from web URLs which then extracts the files of interest to a separate folder. I have the .Zip files folder deleted from the working directory after the .CSV files are extracted to reduce the total resource requirements of the project.

```{r}

# Lists of download / zips / file addresses. 

durls <-
    sprintf("https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip",
            202301:202312)

tempZipPaths <- sprintf("tempZips/%d-divvy-tripdata.zip",
                     202301:202312)

fileNames <- sprintf("%d-divvy-tripdata.csv",
                      202301:202312)

tempfile_paths <- sprintf("tempFiles/%d-divvy-tripdata.csv",
                      202301:202312)

fileList <- sprintf("tripdata/%d-divvy-tripdata.csv",
                       202301:202312)

    
# Need some directories to store the files. 
dir.create("tempZips")

dir.create("tripdata")

dir.create("tempFiles")

# review address info that was just created
tibble::tibble("URLs" = durls,
               "Zip File Paths" = tempZipPaths,
               "File Names" = fileNames,
               "Parquet File Paths" = tempfile_paths)

```

### File Downloads

```{r}

curl::multi_download (durls,
                      destfiles = tempZipPaths)
```

### Unzip, Convert and Relocate

I wanted to reduce resource requirements by converting the .CSV files to .Feather so that there are no unneeded local copies of data. I don't want copies of .CSV files to stick around. The project is just about 1 GB for one year of data. It's recommended to use .Feather format for lighter projects, \~1GB or less, so felt that was a respectable choice. I may change to .Parquet if it becomes an issue.

[R for Data Science: Chapter 22: Arrow](https://r4ds.hadley.nz/arrow "Arrow")

```{r}

# A custom function that makes unzipping, converting and relocating files all at once, simple. 
unz_relocate <- function (x = tempfile_paths,
                          y = tempZipPaths,
                          z = fileNames) {
    for (i in seq(x)) {
        
        utils::unzip (y[i],
                      z[i])
        file.rename (z[i],
                     x[i])
    }
}

unz_relocate()

unlink("tempZips",
       recursive = TRUE)
```

```{r}

tripTibble <- purrr::map(tempfile_paths[1:12],
                         arrow::read_csv_arrow) |>
    purrr::list_rbind()

original_nobs <- nrow(tripTibble)

tripTibble <- tripTibble |>
    tidyr::drop_na()

complete_nobs <- nrow(tripTibble)

textvector <- c(" Count before dropping incomplete rows: ",
                "Count after dropping incomplete rows: ")

cat(textvector[1], original_nobs, 
    "\n \n",
    textvector[2], complete_nobs
    )
```

```{r}

tripTibble |>
    dplyr::select(started_at, ended_at) |>
    dplyr::transmute("trip_time" =
                         lubridate::time_length(
                             lubridate::interval(started_at,
                                                 ended_at),
                             unit = "minute"))
```

```{r}


tripTibble |> arrow::write_dataset("tempFiles",
                                   existing_data_behavior = "delete")

fileList <- list.files(path = "tempFiles",
                              full.names = TRUE,
                              recursive = TRUE)

fileList
```

```{r}

tripset <- arrow::open_dataset(sources = fileList[1],
                               format = "parquet") |>
    dplyr::group_by(month = lubridate::month(started_at),
                    weekday = lubridate::wday(started_at),
                    hour = lubridate::hour(started_at)) |>
    dplyr::select(
        ride_id,
        rideable_type,
        started_at,
        start_station_name,
        start_station_id,
        ended_at,
        end_station_name,
        end_station_id,
        member_casual,
        month,
        weekday,
        hour
    ) |>
   dplyr::ungroup()


tripset |>
    arrow::to_duckdb() |>
    lubridate::interval(
        lubridate::hms(started_at),
        lubridate::hms(ended_at)) |>
    dplyr::transmute("rideDuration" = difftime(
        lubridate::hms(started_at),
        lubridate::hms(ended_at))) |>
    arrow::to_arrow() |>
    dplyr::collect()

tripset |>
    arrow::to_duckdb() |>
    dplyr::ungroup() |>
    dplyr::select("ride_id") |>
    dplyr::count() |>
    arrow::to_arrow() |>
    dplyr::collect()

tripset |>
    arrow::to_duckdb() |>
    dplyr::count() 

tripset |> arrow::write_dataset(path = "tripdata",
                                format = "parquet",
                                existing_data_behavior = "delete")

unlink("tempFiles",
       recursive = TRUE)

tripset <- arrow::open_dataset(sources = "tripdata",
                              format = "parquet")

# Wanted to verify where the extracted data is at. 
fileList <- list.files(path = "tripdata",
                              full.names = TRUE,
                              recursive = TRUE)

fileList
```

[Why DuckDB?](https://duckdb.org/why_duckdb)

```{r}

dbconn <- DBI::dbConnect(duckdb::duckdb())

# For querying the arrow dataset with the benefits of an OLAP database. 
duckdb::duckdb_register_arrow(dbconn,
                              "trip_data",
                              tripset)
```

```{r}

ride_id,
rideable_type,
started_at,
start_station_name,
start_station_id,
ended_at,
end_station_name,
end_station_id,
member_casual,
month
```

We started with 5,719,877 observations for the time period of Jan-Dec 2023, then removed 1,388,170 of those original observations which were incomplete.

Maybe we only need to check the ride_id column? Initially, the assumption that 4,331,707 is what should be expected as total unique observations. Perhaps it's not that easy, though? Checking the other possible parameters might be worth the time.

```{r}


doubleDecimalTibble <- tripset |>
    dplyr::select(end_station_id) |>
    as.data.frame() |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[1-9]$")) |>
    dplyr::arrange(end_station_id) |>
    arrow::as_arrow_array() |>
    tibble::as_tibble()



# This is a separate table used to analyze the observations returned as not distinct (n > 1). This adds an extra column labeled "n".
dupeTable <- tripset |>
    arrow::to_duckdb() |>
    dplyr::select(started_at:end_station_name) |>
    # Counts of unique rows added for column 'n'
    dplyr::add_count(started_at,
                     ended_at,
                     start_station_name,
                     end_station_name) |>
    # Only observations that have been duplicated 1 or more 
    # times are shown
    dplyr::filter(n > 1) |>
    # We want to see all rows, not just one row for each obs
    dplyr::ungroup() |>
    dplyr::arrange(started_at) |>
    arrow::to_arrow() |>
    dplyr::collect() |>
    arrow::as_arrow_array() |>
    tibble::as_tibble()
```

Of the other columns, it seems that the start_time, end_time, start_station, and end_station could show if there are possibly hidden duplicated observations.

I assumed that having the exact same times/dates and stations for two different ride IDs is a mistake. Although, I do not know how that error would happen. I could have assumed one person could check out multiple cycles at once. That, however, has only happened 18 times over the course of a full year.

```{r}

n <- dupeTable |> 
    dplyr::distinct(n) |>
    as.integer()

cat("All distinct values of duplicates: ", 
    n)
```

```{r}

cat("Total observations that have and are duplicates: ",
       length(dupeTable[[1]]))
```

By applying distinct() on dupeTable, we see the only distinct value is 2. We can safely conclude that, of the duplicates, each has a mininum and maximum of 1 extra copy.

Number of rows in the dupeTable is 36. Because each duplicated observation has one duplicate (n = 2), expected removed nobs is 18.

```{r}

# The issue is, we need to get rid of not all of these rows, but just the extra duplicate observations. 

# If there were 2 rows of duplicates, we would want to end up with 1 row after removing the extras.
undupedTable <- dupeTable |>
    arrow::to_duckdb() |>
    dplyr::distinct(started_at,
                     start_station_name,
                     ended_at,
                     end_station_name,
                     .keep_all = TRUE) |>
    arrow::to_arrow() |>
    dplyr::collect() |>
    arrow::as_arrow_array() |>
    tibble::as_tibble()
```

```{r}

n <- undupedTable |>
    dplyr::select(started_at) |>
    dplyr::distinct() |>
    dplyr::count() |>
    as.integer()

cat("Count of distinct observations: ", n)
```

The count of observed distinct values for the un-duplicated table was indeed 18.

```{r}

# Run a count on how many rows or observations there are in the dataset.
incorrectDistinct <- tripset |>
    arrow::to_duckdb() |>
    dplyr::distinct(dplyr::pick("ride_id")) |>
    dplyr::count(name = "Incorrect Distinct Observations") |>
    arrow::to_arrow() |>
    dplyr::collect() |>
    arrow::as_arrow_array() |>
    tibble::as_tibble() |>
    as.integer()

cat("The incorrect count of distinct observations:", incorrectDistinct)
```

```{r}


correctDistinct <- tripset |>
    arrow::to_duckdb() |>
    dplyr::distinct(
        dplyr::pick(
            "started_at",
            "start_station_name",
            "ended_at",
            "end_station_name"
        )
    ) |>
    dplyr::count() |>
    arrow::to_arrow() |>
    dplyr::collect() |>
    arrow::as_arrow_array() |>
    tibble::as_tibble() |>
    as.integer()

cat("The corrected count of distinct observations we expect to see: ", correctDistinct)
```

```{r}
tripset |>
    arrow::to_duckdb() |>
    dplyr::distinct(
        dplyr::pick(
            "started_at",
            "start_station_name",
            "ended_at",
            "end_station_name"
        )
    ) |>
    arrow::to_arrow() |>
    dplyr::collect() |>
    arrow::as_arrow_array() |>
    tibble::as_tibble() |>
    dplyr::glimpse()
```

```{r}

correctionTibble <-
    tibble::tibble(
        Incorrect= incorrectDistinct,
        Correct = correctDistinct,
        Removed = Incorrect - Correct
    )

correctionTibble
```

The incorrect number of observations (nobs) was 4,331,707. The correct nobs after removing duplicated obs was 4,331,689. In short, 18 additional obs were removed.

```{r}

tempTibb <- tripset |>
    dplyr::select( ride_id:month) |>
    arrow::to_duckdb() |>
    dplyr::distinct(started_at, 
                    start_station_name, 
                    ended_at,
                    end_station_name, 
                    .keep_all = TRUE) |>
    arrow::to_arrow() |>
    dplyr::collect() |>
    arrow::as_arrow_array() |>
    tibble::as_tibble()

    
tempTibb |>
    arrow::write_dataset(
        path = "tripdata",
        format = "feather",
        partitioning = "month",
        existing_data_behavior = "overwrite"
    )

```

```{r}

n <-  nrow(tabledf)
cat("Correct actual observation count: ", n)
```

The actual observation count after applying distinct() on the 4 parameters listed aligns with what we predicted.

```{r}

# sorting the entire table by the start_at column
tabledf <- tabledf |>
    dplyr::arrange(started_at) |>
    arrow::as_arrow_array() |>
    tibble::as_tibble()

dplyr::glimpse(tabledf)
```

```{r}

duckdb::duckdb_unregister(dbconn,
                          "tripset")

tempdf |> arrow::write_parquet(sink = "temps")

rm(tempdf)

?arrow::as_data_type()

tripset <- arrow::open_dataset(sources = "temps",
                               format = "parquet") |>
    dplyr::group_by(month = lubridate::month(started_at))

tripset |> arrow::write_dataset(path = "tripdata",
                                format = "feather",
                                partitioning = "month",
                                existing_data_behavior = "delete")

unlink("temps")

tripset <- arrow::open_dataset(sources = "tripdata",
                               format = "feather")


#duckdb::duckdb_register_arrow(dbconn, 
#                              "tripset")

tripset |>
    dplyr::glimpse()
```

```{r}

decimalTibble <- tripset |>
    dplyr::select(end_station_id) |>
    as.data.frame() |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id, "\\.[:digit:]$")) |>
    dplyr::count() |>
    arrow::as_arrow_array() |>
    tibble::as_tibble()

check1079 <- tripset |>
    dplyr::select(end_station_id) |>
    as.data.frame() |>
    dplyr::collect() |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "[1][0][7][9]")) |>
    dplyr::count() |>
    arrow::as_arrow_array() |>
    tibble::as_tibble()


doubleDecimalTibble <- tripset |>
    dplyr::select(end_station_id) |>
    as.data.frame() |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[1-9]$")) |>
    dplyr::arrange(end_station_id) |>
    arrow::as_arrow_array() |>
    tibble::as_tibble()


removes_oneDecimalNum <- tripset |>
    dplyr::select(end_station_id) |>
    dplyr::collect() |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::mutate(
        end_station_id = stringr::str_replace(
            string = end_station_id,
            pattern = "\\.[:digit:]$",
            replacement = "")
        ) |>
    arrow::as_arrow_array() |>
    tibble::as_tibble()

removes_oneDecimalNum
```

It doesn't really seem like the station IDs containing decimals (e.g., ####.# or ####.#.#) were typos. I tried asking if station IDs containing decimal points are meant to be that way. There were no station ids with decimal numbers other than 0 or 1 after the decimal place. There were many different station IDs with one 0 after the decimal. There was only 1 station ID with a 1 following the decimal and it was in the form of "###.1.1".

You can read in the above link why I would choose to use DuckDB and OLAP for this project.

```{r}

# Open .Parquet file dataset connection pointer
dataquet <- arrow::open_dataset(sources = "parquets",
                                format = "parquet")
# Establish a new connection to a virtual in memory DuckDB database
duckCon <- DBI::dbConnect(duckdb::duckdb())

# Register the dataset connection object to a location in the DDB
duckdb::duckdb_register_arrow(duckCon,
                              "duckbase",
                              dataquet)
```

```{r}

# Confirm table info
dplyr::tbl(duckCon, "duckbase") |>
         dplyr::glimpse()
```

## SQL Queries

```{r}

sql_countRiders <- paste0(
    "SELECT COUNT(*) AS 'Distinct Riders' ",
    "FROM (",
    "SELECT DISTINCT ride_id ",
    "FROM trip_data)"
)

qres_countRiders <- duckdb::dbSendQuery(conn = dbconn,
                                        statement = sql_countRiders)

fetch_countRiders <- duckdb::dbFetch(qres_countRiders)

fetch_countRiders

duckdb::dbClearResult(qres_countRiders)
```

```{r}

# Makes formatting simpler for SQL syntax
sql_selectHead <- paste0("SELECT(*) ",
                         "FROM duckbase ",
                         "LIMIT 10")

# Query result variable
qres_selectHead <- duckdb::dbSendQuery(conn = duckCon,
                                       sql_selectHead)
# Results of query in the form of a dataframe
duckdb::dbFetch(qres_selectHead)
# To save on overall memory overhead
duckdb::dbClearResult(qres_selectHead)
```

```{r}

rCounts_types <- dplyr::tbl(duckCon, "duckbase") |>
    dplyr::group_by(rideable_type) |>
    dplyr::summarize(rideTypecount = count(), .groups = "keep")

groupCount_rideTypes
#or this
sql_ridetypeCounts <- paste0(
    "SELECT rideable_type, count() AS 'rt_counts' ",
    "FROM duckbase ",
    "GROUP BY rideable_type"
)

qres_ridetypeCounts <- duckdb::dbSendQuery(conn = duckCon,
                                           statement = sql_ridetypeCounts
 )

fres_rideTypeCounts <- duckdb::dbFetch(qres_ridetypeCounts)
#duckdb::dbClearResult(qres_ridetypeCounts)
```

```{r}

fres_rideTypeCounts
dplyr::glimpse(fres_rideTypeCounts)

newPlot <- ggplot2::ggplot(data = fres_rideTypeCounts,
                           mapping = ggplot2::aes(
                               x = rideable_type,
                               y = rt_counts)) +
    ggplot2::geom_col(fill = "red")

newPlot
```

```{r}

dplyr::tbl(duckCon, "duckbase") |>
    dplyr::group_by(start_station_id) |>
    dplyr::summarize(StationCount = count()) |>
    dplyr::arrange(StationCount, start_station_id)
```

With DuckDB, approx 0.11s to return query results.

```{r}

dataquet |>
    dplyr::collect() |>
    dplyr::distinct(ride_id) |>
    dplyr::count(name = "Distinct Ride ID's")
```

In memory parquet file query took around 2.55s. Personal observation, increased memory usage by about 1GB for non-DuckDB queries and around 0.2GB for DuckDB (DDB) queries. Clear advantage goes to: DDB.

Running SQL syntax to the DDB has the same results as dplyr package syntax. These are relatively faster and more memory efficient than querying an in memory dataframe.

```{sql, connection = dbconn, output.var = "Distinct"}

SELECT COUNT 
    (DISTINCT ride_id) AS "Distinct Obs"
FROM
    duckbase
```

```{sql, connection = dbconn, output.var = "MonthlyRiders"}

SELECT MonthNum, AVG(COUNT) AS RidesPerMonth
FROM (
    SELECT duckbase.*,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
)
GROUP BY MonthNum
ORDER BY RidesPerMonth
```

```{sql, connection = dbconn, output.var = "MonthlyAVG"}

SELECT AVG(RidesPerMonth) AS "MonthlyAVG"
FROM (
  SELECT MonthNum, COUNT() AS RidesPerMonth
  FROM (
    SELECT
      duckbase.*,
      EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
  )
  GROUP BY MonthNum
)
```

```{sql, connection = dbconn, output.var = "sql_counts_month_rides"}

SELECT month, COUNT(member_casual) AS Rider_count
FROM duckbase
GROUP BY month
ORDER BY month
```

```{sql, connection = dbconn, output.var = "sql_counts_month_rides"}

SELECT month, COUNT(member_casual) AS Rider_count
FROM duckbase
GROUP BY month
ORDER BY month
```

# Descriptive Statistics

#### Stats: Monthly Average

```{sql, connection = dbconn, output.var = "sql_avg_month_rides"}

SELECT AVG(RidesPerMonth) AS "MonthlyAVG"
FROM (
  SELECT MonthNum, COUNT() AS RidesPerMonth
  FROM (
    SELECT
      duckbase.*,
      EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
  )
  GROUP BY MonthNum
)
```

```{r}

cat(stringr::str_c(as.integer(MonthlyAVG)))
```

```{sql, connection = duckCon, output.var = "sql_counts_month_casual"}

SELECT MonthNum, COUNT(member_casual) AS Casuals
FROM (
  SELECT
    started_at,
    member_casual,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
  FROM duckbase
  WHERE (member_casual = 'casual')
)
GROUP BY MonthNum
```

#### Average Casuals Per Month

```{sql, connection = duckCon, output.var = "sql_AVG_casual"}

SELECT AVG(Casuals) AS AvgCasuals
FROM (
  SELECT MonthNum, COUNT() AS Casuals
  FROM (
    SELECT
      started_at,
      member_casual,
      EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
    WHERE (member_casual = 'casual')
  )
  GROUP BY MonthNum
)
```

```{r}

cat(stringr::str_c(as.integer(AVG_casuals_monthly)))
```

# testing section

```{sql, connection = duckCon, output.var = "sql_counts_month_casual_e"}

SELECT MonthNum, COUNT(member_casual) AS CasualsOnElectric
FROM (
  SELECT
    started_at,
    member_casual,
    rideable_type,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
  FROM duckbase
  WHERE (member_casual = 'casual') AND (rideable_type = 'electric_bike')
)
GROUP BY MonthNum
```

```{sql, connection = duckCon}

CREATE TABLE tbl_eMembers AS
SELECT MonthNum, COUNT(member_casual) AS MembersOnElectric
FROM (
  SELECT
    started_at,
    member_casual,
    rideable_type,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
  FROM duckbase
  WHERE (member_casual = 'member') AND (rideable_type = 'electric_bike')
)
GROUP BY MonthNum
```

```{sql, connection = duckCon}

CREATE TABLE tbl_eCasuals AS 
SELECT MonthNum, COUNT(member_casual) AS CasualsOnElectric
FROM (
  SELECT
    started_at,
    member_casual,
    rideable_type,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
  FROM duckbase
  WHERE (member_casual = 'casual') AND (rideable_type = 'electric_bike')
)
GROUP BY MonthNum
```

```{r}

duckdb::duckdb_list_arrow (duckCon)
    
duckdb::dbListTables(duckCon)
    
dplyr::tbl(duckCon, "tbl_eCasuals") |>
        dplyr::select_all()
    
dplyr::tbl(duckCon, "tbl_eMembers") |>
    dplyr::select_all()
```

```{sql, connection = duckCon}

SELECT tbl_eMembers.*, CasualsOnElectric
FROM tbl_eMembers
LEFT JOIN tbl_eCasuals
  ON (tbl_eMembers.MonthNum = tbl_eCasuals.MonthNum)
```

### One Big Table?

```{r}

duckdb::duckdb_list_arrow(duckCon)

rAvg_riders <-
    dplyr::tbl(duckCon,
               "duckbase") |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::summarize(RidesPerMonth = count()) |>
    dplyr::summarize(AvgRiders = mean(RidesPerMonth))

rAvg_casuals <-
    dplyr::tbl(duckCon,
               "duckbase") |>
    dplyr::select(started_at, member_casual) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "casual") |>
    dplyr::summarize(Casuals = count()) |>
    dplyr::summarize(AvgCasuals = mean(Casuals))


rCounts_types

rAvg_riders
rAvg_casuals
```

##### Counts_By_Month

```{r}

# Create a monthly count summaries table to be joined by other tables
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::summarize(RidesPerMonth = count()) |>
    dplyr::arrange(MonthNum) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month")
```

Verifying the new table and data

```{r}

dplyr::tbl(duckCon, "Counts_By_Month") |>
    tibble::view(n = 12)
```

Adding a new column by joining a dataframe object to the Counts_By_Month table by month.

```{r}

# monthly casual counts
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "casual") |>
    dplyr::summarize(Casuals = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month_Casuals")
```

```{r}

dplyr::tbl(duckCon, "Counts_By_Month::Casual Riders") |>
    tibble::view(n = 12)
```

```{r}

dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at, member_casual) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "member") |>
    dplyr::summarize(Annuals = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month_Annuals")
```

```{r}

dplyr::tbl(duckCon, "Counts_By_Month::Annual Members") |>
    tibble::view(n = 12)
```

```{r}

duckdb::dbListTables(duckCon)
```

```{r}

# create a monthly casual ebike riders table
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual,
                  rideable_type) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "casual",
                  rideable_type == "electric_bike") |>
    dplyr::summarize(Casual_eBikers = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month_Casual_eBikers")
```

```{r}

dplyr::tbl(duckCon, "Counts_By_Month::Casual eBikers") |>
    tibble::view(n = 12)
```

```{r}

# create a monthly annual member ebike riders table
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual,
                  rideable_type) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "member",
                  rideable_type == "electric_bike") |>
    dplyr::summarize(Annual_eBikers = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month_Annual_eBikers")
```

```{r}

dplyr::tbl(duckCon, "Counts_By_Month::Annual Member eBikers") |>
    tibble::view(n = 12)
```

```{r}

# create a monthly annual member classic riders table
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual,
                  rideable_type) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "member",
                  rideable_type == "classic_bike") |>
    dplyr::summarize(Annual_classicBikers = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = 
                             "Counts_By_Month_Annual_classicBikers")
```

```{r}

dplyr::tbl(duckCon, 
           "Counts_By_Month::Annual Members::Classic Bikers") |>
    tibble::view(n = 12)
```

```{r}

# create a table of monthly counts for casual riders on classic bikes
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual,
                  rideable_type) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "casual",
                  rideable_type == "classic_bike") |>
    dplyr::summarize(casual_classicBikers = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = 
                             "Counts_By_Month_Casual_classicBikers")
```

```{r}

dplyr::tbl(duckCon, 
           "Counts_By_Month::Casuals::Classic Bikers") |>
    tibble::view(n = 12)
```

```{r}
duckdb::dbListTables(duckCon)
```

```{r}

dplyr::tbl(duckCon,
           "Counts_By_Month") |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Casuals")) |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Annuals")) |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Casual_eBikers")) |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Annual_eBikers")) |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Casual_classicBikers"))  |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Annual_classicBikers"))
```

```{r}


dplyr::tbl(dbconn,
           "trip_tbl") |>
    dplyr::group_by(rideable_type) |>
    dplyr::summarize(rideTypecount = count(),
                     .groups = "keep")
```

```{r}
dplyr::tbl(dbconn,
           "trip_tbl") |>
    dplyr::select(ride_id) |>
    dplyr::count()
```

```{r}
?dplyr::tbl

colHeaders <- duckdb::dbListFields(dbconn,
                     "trip_tbl")

as.list(colHeaders)


colheads <- dplyr::tbl(dbconn,
           "trip_tbl") |>
    colnames()

naList <- dplyr::tbl(dbconn,
           "trip_tbl") |>
    dplyr::select(ride_id) |>
    as.list()

naList |>  purrr::map_lgl(!is.na("ride_id"))

naList |> purrr::detect(is.na())

x1 <- list(naList)

naList <- naList |>
    dplyr::pull(ride_id)

naList |>
    purrr::detect(is.na())

colheads


#    dplyr::select(ride_id:month) |>
#    dplyr::filter(!is.na(started_at)) |>
#    dplyr::distinct() |>
#    dplyr::count(name = "Distinct Observations") 

countRows <- paste0("SELECT COUNT(*) AS 'Observations' ",
                    "FROM trip_tbl")

resCount <- duckdb::dbSendQuery(dbconn, countRows)
fetchCount <- duckdb::dbFetch(resCount)
fetchCount
```

```{r}

dplyr::tbl(dbconn,
           "trip_tbl") |>
    as.data.frame() |>
    dplyr::glimpse()
```

```{r}
sql_countRideIds <- paste0(
    "SELECT COUNT(*) AS 'Distinct Ride IDs' ",
    "FROM (",
    "SELECT DISTINCT ride_id ",
    "FROM trip_data
    )"
)


qres_countRideIds <- duckdb::dbSendQuery(conn = dbconn,
                                        statement = sql_countRideIds)

fetch_countRiders <- duckdb::dbFetch(qres_countRideIds)

fetch_countRiders

duckdb::dbClearResult(qres_countRideIds)
```

```{r}
# With list_rbind(), map can be used to create a dataframe from a list of files
#oneTibble <- purrr::map(csvFileList[1:12], 
 #                       read_csv) |>
#   purrr::list_rbind ()

```

```{r}

# Keeping track of observations/rows dropped.
arrow::write_dataset(format = "parquet") |>
    dplyr::group_by(month = lubridate::month(started_at))

```

```{r}
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at, member_casual) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "member") |>
    dplyr::summarize(Annuals = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month_Annuals")
```
