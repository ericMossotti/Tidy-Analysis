---
title: "Tidy Analysis"
author: "Eric Mossotti"

code-fold: true

df_print: "tibble" 

format: html
toc: true
---

```{r include = FALSE}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# Data Cleaning: The Heavy Lifting

In the spirit of thriftiness, I hypothesize that data analysis could be made more accessible to individuals and businesses if done a certain way. At first, it would seem that data analysis would be an expensive endeavor but as I learn and think about it more, I am not sure why that has to be the case. Either way, this project is meant as an exercise in professional data analysis where time and money saved makes a difference.

```{r}
durls <-
    sprintf("https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip",
            202301:202312)

zipPaths <- sprintf("tripZips/%d-divvy-tripdata.zip",
                     202301:202312)

filePaths <- sprintf("tripdata/%d-divvy-tripdata.csv",
                      202301:202312)

zipNames <- sprintf("%d-divvy-tripdata.zip",
                     202301:202312)

fileNames <- sprintf("%d-divvy-tripdata.csv",
                      202301:202312)

pq_path <- "pq_tripdata"

dir.create("tripZips")

dir.create("tripdata")

dir.create("pq_tripdata")

curl::multi_download (durls,
                      destfiles = zipPaths)

unz_csv <- function (x = filePaths,
                     y = zipPaths,
                     z = fileNames) {
    
    for (i in seq(x)) {
        utils::unzip (y[i],
                      z[i])
        
        file.rename (z[i],
                     x[i])
    }
}
```

```{r}
csvFileList <- list.files(path = "tripdata",
                          full.names = TRUE)

# view {tibble}
matrix(csvFileList,
       dimnames = list(seq(csvFileList),
                       c("csvfiles")))
```

```{r}

# map {purrr}, 

# With list_rbind(), map can be used to directly create a dataframe from a list of files
oneTibble <- purrr::map (csvFileList[1:12], read_csv) |>
    purrr::list_rbind ()
```

( In R a tibble is synonymous with a data frame )

```{r}

typeof(oneTibble)

is_tibble(oneTibble)

is.data.frame(oneTibble)
```

```{r}
oneTibble[1,]
```

```{r}
originalLength <- length(oneTibble[[1]])

print(originalLength)
```

We started with 5,719,877 observations for the time period of Jan-Dec 2023, then drop_na() removed 1,388,170 of those observations.

```{r}

# drop_na {tidyr}	
oneTibble <- tidyr::drop_na(oneTibble)
```

```{r}
tibble::tibble(
    Complete_Observations = length (oneTibble [[1]]),
    "Rows Deleted by drop_na()" = originalLength - Complete_Observations
)
```

```{r}

# Maybe only need to check ride_id column? Actually, no.

# pick {dplyr}	
dplyr::distinct(oneTibble, dplyr::pick ("ride_id")) |>
    dplyr::count(name = "Distinct Ride ID's")
```

Initially, the assumption that 4,331,707 is what should be expected as total unique observations. Perhaps it's not that easy, though? Checking the other possible parameters might be worth the time.

Of the other columns, it seems that the start_time, end_time, start_station, and end_station could show if there are hidden duplicated observations. There is no mention that more than one bike can be checked out at once. One big limitation of this analysis is that it's based on a fictional but realistic dataset.

For the sake of this exercise it's assumed that due the logistics of having the exact same times/dates and stations for two different riders is impossible. The other option would be to assume one person could check out and in multiple cycles. It's only happened 18 times over the course of a full year. In the real world, we would want to be absolutely sure, but for this exercise, it is reasonable to assume these are errors.

```{r}

# This is a separate table used to analyze the observations returned as not distinct (n > 1). This adds an extra column (n) so didn't want to mess with the main dataframe for this.

dupTibble <- oneTibble |>
    dplyr::select(started_at:end_station_name) |>
    # adds a count of rows for column 'n'
    dplyr::add_count(started_at,
                     ended_at,
                     start_station_name,
                     end_station_name) |>
    # so that only observations that have been duplicated 1 or more         times are shown
    dplyr::filter(n > 1) |>
    # because we want to see all the rows, not just one row for each obs
    dplyr::ungroup() |>
    dplyr::arrange(started_at)
```

```{r}

print(dupTibble)

paste0("Number of observations with duplicates: ",
       length(dupTibble[[1]]))

dplyr::distinct(dupTibble["n"])
```

By applying distinct() on dupTibble, we see the only distinct value is 2. We can safely conclude that of the duplicates each duplicated observation has only 1 extra copy.

```{r}
# verify how many rows have/are duplicates
nrow(dupTibble)
```

Number of rows in the dupTibble is 36. Because each duplicated observation has one duplicate (n = 2), expected removed nobs is 18.

```{r}

# issue is, we need to get rid of not all of these rows, but just the extra duplicate observations. 

# If there were 2 rows of duplicates, we would want to end up with 1 row after removing the extras.

undupdTibble <- dupTibble |>
    
    dplyr::distinct(started_at,
                     start_station_name,
                     ended_at,
                     end_station_name,
                     .keep_all = TRUE)
```

```{r}
print(undupdTibble)
```

```{r}
nrow(undupdTibble)
```

Our predicted value for observations in the undupdTibble was indeed 18.

```{r}
uncorrectDistinct <-
    dplyr::distinct(oneTibble, dplyr::pick("ride_id")) |>
    dplyr::count(name = "Uncorrected Distinct Observations")

uncorrectDistinct
```

```{r}
correctDistinct <-
    dplyr::distinct(
        oneTibble,
        dplyr::pick(
            "started_at",
            "start_station_name",
            "ended_at",
            "end_station_name"
        )
    ) |>
    dplyr::count(name = "Corrected Distinct Observations")

correctDistinct

correctionTibble <-
    tibble::tibble(
        Uncorrected = uncorrectDistinct[[1]],
        Expected_Corrected = correctDistinct[[1]],
        Removed_Obs = Uncorrected - Corrected
    )

correctionTibble
```

Uncorrected number of observations (nobs) after drop_na() was 4,331,707. The corrected nobs after removing one copy of the duplicates since all of them had only one duplicate, resulted in 4,331,689 nobs. By applying distinct() on the tibble for 4 parameters at once, 18 observations were removed.

```{r}
oneTibble <- oneTibble |>
    dplyr::distinct(started_at, 
                    start_station_name, 
                    ended_at,
                    end_station_name, 
                    .keep_all = TRUE)
```

```{r}
nrow(oneTibble)
```

```{r}
# sorting the entire table by the start_at column
oneTibble <- oneTibble |>
    dplyr::arrange(started_at)
```

# Data Prep: Lighter Lifting with Apache Arrow

## Parquet Filesystem

The multi-file partitioning of the dataset should result in faster queries. Parquet files require less long term storage space than CSV's as well.

[R for Data Science: Chapter 22: Arrow](https://r4ds.hadley.nz/arrow "Arrow")

```{r}
# from CSV's, now a single dataframe, to partioned parquet files?

# Just needed the tidied tibble out of memory and into an actual file. 
arrow::write_parquet(oneTibble, sink = "oneQuet")

dataquet <- arrow::open_dataset("oneQuet",
                                format = "parquet")

dataquet <- dataquet |>
    dplyr::group_by(month = lubridate::month(started_at))

dataquet |>
    arrow::write_dataset(path = "parquets",
                         format = "parquet")
    
dataquet <- arrow::open_dataset(sources = "parquets",
                              format = "parquet")

csvset <- arrow::open_dataset(sources = "tripdata",
                              format = "csv")

onecsvset <- csvset |>
    arrow::write_csv_arrow(sink = "oneCSVfile")

onecsvset <- arrow::open_csv_dataset(sources = "oneCSVfile")
```

```{r}
arrow::to_duckdb(
    dataquet,
    con = arrow_duck_connection(),
    table_name = unique_arrow_tablename(),
    auto_disconnect = TRUE
)
```

```{r}
duckdb::dbListTables(duckCon)
```

```{r}
onecsvset |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

```{r}

csvset |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

```{r}
dataquet |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

## DuckDB: In memory OLAP Database

[Why DuckDB?](https://duckdb.org/why_duckdb)

You can read in the above link why I would choose to use DuckDB for this project.

```{r}

dataquet <- arrow::open_dataset(sources = "parquets",
                                format = "parquet")

duckCon <- DBI::dbConnect(duckdb::duckdb())

duckdb::duckdb_register_arrow(duckCon,
                              "duckbase",
                              dataquet)

#duckdb::duckdb_register(duckCon,
#                        "bigDuck",
#                        duckBase)

#dplyr::tbl(duck)
```

```{r}

sql_countRiders <- paste0(
    "SELECT COUNT(*) AS 'Distinct Riders' ",
    "FROM (",
    "SELECT DISTINCT ride_id ",
    "FROM duckbase)"
)

qres_countRiders <- duckdb::dbSendQuery (conn = duckCon,
                                         statement = sql_countRiders)

fetch_countRiders <- duckdb::dbFetch(qres_countRiders)

fetch_countRiders

duckdb::dbClearResult(qres_countRiders)
```

```{r}

sql_selectHead <- paste0("SELECT(*) ",
                         "FROM duckbase ",
                         "LIMIT 10")

qres_selectHead <- duckdb::dbSendQuery(conn = duckCon,
                                       sql_selectHead)

duckdb::dbFetch(qres_selectHead)

duckdb::dbClearResult(qres_selectHead)
```

```{r}
countObj <- dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::group_by(rideable_type) |>
    dplyr::summarize(count())
```

```{r}
dplyr::tbl(duckCon,
           "duckbase") |>
    #dplyr::group_by("Month" = lubridate::month(lubridate::as_datetime(started_at))) |>
    dplyr::group_by(month) |>
    dplyr::summarize(newvar = count()) |>
    dplyr::arrange(desc(newvar))
```

```{r}
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::pull(rideable_type)
```

```{r}
dplyr::tbl(duckCon, "duckbase") |>
    dplyr::group_by(start_station_id) |>
    dplyr::summarize(StationCount = count(), .groups = "keep") |>
    dplyr::arrange(desc(StationCount), desc(start_station_id))
```

With DuckDB, approx 0.11s to return query results.

```{r}
dataquet |>
    dplyr::collect() |>
    dplyr::distinct(ride_id) |>
    dplyr::count(name = "Distinct Ride ID's")
```

In memory parquet file query took around 2.55s. Personal observation, increased memory usage by about 1GB for non-DuckDB queries and around 0.2GB for DuckDB (DDB) queries. Clear advantage goes to: DDB.

Running SQL syntax to the DDB has the same results as dplyr package syntax. Those are relatively faster and more memory efficient.

```{sql, connection = duckCon, output.var = "Distinct"}

SELECT COUNT 
    (DISTINCT ride_id) AS "Distinct Obs"
FROM
    duckbase
```

```{r}
Distinct
```
