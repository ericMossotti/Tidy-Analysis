---
title: "Tidy Analysis"
author: "Eric Mossotti"

code-fold: true

df_print: "tibble" 

format: html
toc: true
---

```{r include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# Data Cleaning with R and SQL

In the spirit of thriftiness, I hypothesize that data analysis could be made more accessible to individuals and businesses. At first, it would seem that data analysis would be an expensive endeavor but as I learn and think about it more, I am not sure why that has to be the case. Either way, this project is meant as an exercise in professional data analysis where time and money saved makes a difference.

I think .ZIP files would be commonplace as they substantially reduce long term storage requirements. Here, I wrote a short function that downloads a set of zip files from web URLs and then extracts the files of interest to a separate folder.

```{r}
# Efficiently create a list of download addresses. 
durls <-
    sprintf("https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip",
            202301:202312)

zipPaths <- sprintf("tripZips/%d-divvy-tripdata.zip",
                     202301:202312)

# Define file paths for .CSVs. 
filePaths <- sprintf("tripdata/%d-divvy-tripdata.csv",
                      202301:202312)

# Define filenames for the .ZIP and .CSV files. 
zipNames <- sprintf("%d-divvy-tripdata.zip",
                     202301:202312)

# Define filenames for .CSV's
fileNames <- sprintf("%d-divvy-tripdata.csv",
                      202301:202312)

pq_path <- "pq_tripdata"

# I need some directories to store the files. 
dir.create("tripZips")

dir.create("tripdata")

dir.create("pq_tripdata")
```

```{r}
# Need to download several compressed files from URLs to the directory destination and filepaths (zipPaths) previously created.
curl::multi_download (durls,
                      destfiles = zipPaths)
```

```{r}
# A custom function that makes unzipping and moving files easy and fun.
unz_csv <- function (x = filePaths,
                     y = zipPaths,
                     z = fileNames) {
    
    for (i in seq(x)) {
        utils::unzip (y[i],
                      z[i])
        
        file.rename (z[i],
                     x[i])
    }
}
```

```{r}
# Wanted to verify where the extracted data is at. 
csvFileList <- list.files(path = "tripdata",
                          full.names = TRUE)

matrix(csvFileList,
       dimnames = list(seq(csvFileList),
                       c("csvfiles")))
```

```{r}
# With list_rbind(), map can be used to directly create a dataframe from a list of files
oneTibble <- purrr::map (csvFileList[1:12], read_csv) |>
    purrr::list_rbind ()
```

```{r}
oneDT <- do.call(rbind,
                 lapply(csvFileList,
                        data.table::fread))
```

```{r}
oneDT[1,]
```

```{r}
origObserves <- length(oneDT[[1]])

print(originalLength)
```

We started with 5,719,877 observations for the time period of Jan-Dec 2023, then drop_na() removed 1,388,170 of those observations.

```{r}
# Pretty straightforward way to remove incomplete observations.
oneDT <- tidyr::drop_na(oneDT)
```

```{r}
# Keeping track of observations/rows dropped.
tibble::tibble(
    Complete_Observations = length(oneTibble [[1]]),
    "Rows Deleted by drop_na()" = originalLength - Complete_Observations
)
```

```{r}
# Maybe only need to check ride_id column? I wish.
dplyr::distinct(oneTibble, dplyr::pick ("ride_id")) |>
    dplyr::count(name = "Distinct Ride ID's")
```

Initially, the assumption that 4,331,707 is what should be expected as total unique observations. Perhaps it's not that easy, though? Checking the other possible parameters might be worth the time.

Of the other columns, it seems that the start_time, end_time, start_station, and end_station could show if there are hidden duplicated observations. There is no mention that more than one bike can be checked out at once. One big limitation of this analysis is that it's based on a simulated dataset. I wanted to reach a broad audience and I know the Google Data Analytics course community would be able to relate, at the very least. I used the course materials as inspiration, mainly for bootstrapping myself and for organizational purposes.

For the sake of this exercise I have assumed that due the logistics of having the exact same times/dates and stations for two different riders is impossible. Another option would be to assume one person could check out multiple cycles at once. But it has only happened 18 times over the course of a full year. I would think that would happen more often because people probably ride in groups often. Scenarios where one person pays and gets Venmo'd later by their friends, come to mind. In the real world, we would want to be absolutely sure, but for this exercise, it is reasonable to assume these are errors.

```{r}
# This is a separate table used to analyze the observations returned as not distinct (n > 1). This adds an extra column (n) so didn't want to mess with the main dataframe for this.
dupTibble <- oneTibble |>
    dplyr::select(started_at:end_station_name) |>
    # adds a count of rows for column 'n'
    dplyr::add_count(started_at,
                     ended_at,
                     start_station_name,
                     end_station_name) |>
    # so that only observations that have been duplicated 1 or more                   times are shown
    dplyr::filter(n > 1) |>
    # because we want to see all the rows, not just one row for each obs
    dplyr::ungroup() |>
    dplyr::arrange(started_at)
```

```{r}
print(dupTibble)

paste0("Number of observations with duplicates: ",
       length(dupTibble[[1]]))

dplyr::distinct(dupTibble["n"])
```

By applying distinct() on dupTibble, we see the only distinct value is 2. We can safely conclude that of the duplicates each duplicated observation has only 1 extra copy.

```{r}
# verify how many rows have/are duplicates
nrow(dupTibble)
```

Number of rows in the dupTibble is 36. Because each duplicated observation has one duplicate (n = 2), expected removed nobs is 18.

```{r}
# The issue is, we need to get rid of not all of these rows, but just the extra duplicate observations. 

# If there were 2 rows of duplicates, we would want to end up with 1 row after removing the extras.
undupdTibble <- dupTibble |>
    
    dplyr::distinct(started_at,
                     start_station_name,
                     ended_at,
                     end_station_name,
                     .keep_all = TRUE)
```

```{r}
print(undupdTibble)
```

```{r}
nrow(undupdTibble)
```

Our predicted value for observations in the undupdTibble was indeed 18.

```{r}
incorrectDistinct <-
    dplyr::distinct(oneTibble, dplyr::pick("ride_id")) |>
    dplyr::count(name = "Inccorrect Distinct Observations")

incorrectDistinct
```

```{r}
correctDistinct <-
    dplyr::distinct(
        oneTibble,
        dplyr::pick(
            "started_at",
            "start_station_name",
            "ended_at",
            "end_station_name"
        )
    ) |>
    dplyr::count(name = "Corrected Distinct Observations")

correctDistinct
```

```{r}
correctionTibble <-
    tibble::tibble(
        Uncorrected = uncorrectDistinct[[1]],
        Expected_Corrected = correctDistinct[[1]],
        Removed_Obs = Uncorrected - Corrected
    )

correctionTibble
```

Uncorrected number of observations (nobs) after drop_na() was 4,331,707. The corrected nobs after removing one copy of the duplicates since all of them had only one duplicate, resulted in 4,331,689 nobs. By applying distinct() on the tibble for 4 parameters at once, 18 additional observations were removed.

```{r}
oneTibble <- oneTibble |>
    dplyr::distinct(started_at, 
                    start_station_name, 
                    ended_at,
                    end_station_name, 
                    .keep_all = TRUE)
```

```{r}
nrow(oneTibble)
```

```{r}
# sorting the entire table by the start_at column
oneTibble <- oneTibble |>
    dplyr::arrange(started_at)
```

It doesn't really seem like the station IDs containing decimals (e.g., ####.# or ####.#.#) were typos. I tried asking if station IDs containing decimal points are meant to be that way. There were no station ids with decimal numbers other than 0 or 1 after the decimal place. There were many different station IDs with one 0 after the decimal. There was only 1 station ID with a 1 following the decimal and it was in the form of "###.1.1".

# Efficiency Side-Quest

## Parquet Filesystem

The multi-file partitioning of the dataset should result in faster queries. Parquet files require less long term storage space than CSV's as well.

[R for Data Science: Chapter 22: Arrow](https://r4ds.hadley.nz/arrow "Arrow")

### Comparing Parquet and .CSV Performance

```{r}

# Move tibble out of memory and into a long term storage file(s). 
arrow::write_parquet(oneTibble, sink = "oneQuet")

# Create a dataset.
dataquet <- arrow::open_dataset("oneQuet",
                                format = "parquet")

# Create partitioned dataset.
dataquet <- dataquet |>
    dplyr::group_by(month = lubridate::month(started_at))

# Write a partitioned backup filesystem so we don't have to do any of the above steps again moving forward. 
dataquet |>
    arrow::write_dataset(path = "parquets",
                         format = "parquet")

# Dataset needs to point towards the new partitioned parquet file system.
dataquet <- arrow::open_dataset(sources = "parquets",
                              format = "parquet")

# Dataset that points towards our original CSV file system (can it be considered "partitioned"?)  
csvset <- arrow::open_dataset(sources = "tripdata",
                              format = "csv")

# I also wanted to test performance of dataset based on "un-partitioned" .CSV file system
onecsvset <- csvset |>
    arrow::write_csv_arrow(sink = "oneCSVfile")

# Arrow dataset needs to point to the .CSV file
onecsvset <- arrow::open_csv_dataset(sources = "oneCSVfile")
```

We test the performance of a query to a one .CSV file dataset.

```{r}
onecsvset |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

Testing the performance of a query to a "partitioned" .CSV file dataset.

```{r}
csvset |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

Testing the performance of a query to a partitioned .Parquet file dataset.

```{r}
dataquet |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

```{r}
# Need to open a tibble again to do some more testing. Opening from .Parquet file made previously. 
twoTibble <- arrow::read_parquet("oneQuet", as_data_frame = TRUE)

# Having file backups of data comes in handy. 
oneTibble <- readr::read_csv("oneCSVfile")

twoTibble["end_station_id"] |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::count()

#
twoTibble["end_station_id"] |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::count()

#
twoTibble["end_station_id"] |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "[1][0][7][9]")) |>
    dplyr::count()

##
oneTibble["end_station_id"] |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::count()

#
twoTibble["end_station_id"] |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[1-9]$")) |>
    dplyr::arrange(end_station_id)

#
twoTibble["end_station_id"] |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::mutate(
        end_station_id = stringr::str_replace(
            string = end_station_id,
            pattern = "\\.[:digit:]$",
            replacement = ""
        )
    )
```

## DuckDB: In memory OLAP Database

[Why DuckDB?](https://duckdb.org/why_duckdb)

You can read in the above link why I would choose to use DuckDB and OLAP for this project.

```{r}
# Open .Parquet file dataset connection pointer
dataquet <- arrow::open_dataset(sources = "parquets",
                                format = "parquet")
# Establish a new connection to a virtual in memory DuckDB database
duckCon <- DBI::dbConnect(duckdb::duckdb())

# Register the dataset connection object to a location in the DDB
duckdb::duckdb_register_arrow(duckCon,
                              "duckbase",
                              dataquet)

# Viewing some metadata of the new location
dplyr::tbl(duckCon, "duckbase") |>
         dplyr::glimpse()
```

## SQL Queries

```{r}
sql_countRiders <- paste0(
    "SELECT COUNT(*) AS 'Distinct Riders' ",
    "FROM (",
    "SELECT DISTINCT ride_id ",
    "FROM duckbase)"
)

qres_countRiders <- duckdb::dbSendQuery(conn = duckCon,
                                        statement = sql_countRiders)

fetch_countRiders <- duckdb::dbFetch(qres_countRiders)

fetch_countRiders

duckdb::dbClearResult(qres_countRiders)
```

```{r}
# Makes formatting simpler for SQL syntax
sql_selectHead <- paste0("SELECT(*) ",
                         "FROM duckbase ",
                         "LIMIT 10")

# Query result variable
qres_selectHead <- duckdb::dbSendQuery(conn = duckCon,
                                       sql_selectHead)
# Results of query in the form of a dataframe
duckdb::dbFetch(qres_selectHead)
# To save on overall memory overhead
duckdb::dbClearResult(qres_selectHead)
```

```{r}
groupCount_rideTypes <- dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::group_by(rideable_type) |>
    dplyr::summarize(rideTypecount = count(), .groups = "keep") |>

groupCount_rideTypes
#or this
sql_ridetypeCounts <- paste0(
    "SELECT rideable_type, count() AS 'rt_counts' ",
    "FROM duckbase ",
    "GROUP BY rideable_type"
)

sql_ridetypeCounts 

qres_ridetypeCounts <- duckdb::dbSendQuery(
    conn = duckCon,
    statement = sql_ridetypeCounts
 )

fres_rideTypeCounts <- duckdb::dbFetch(qres_ridetypeCounts)
#duckdb::dbClearResult(qres_ridetypeCounts)
```

```{r}
fres_rideTypeCounts
dplyr::glimpse(fres_rideTypeCounts)

newPlot <- ggplot2::ggplot(data = fres_rideTypeCounts,
                           mapping = ggplot2::aes(
                               x = rideable_type,
                               y = rt_counts)) +
    ggplot2::geom_col(fill = "red")

newPlot
```

```{r}
groupCount_month <- dplyr::tbl(duckCon, "duckbase") |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::summarize(RidesPerMonth = count()) |>
    dplyr::arrange(RidesPerMonth)

groupCount_month
```

```{r}
dplyr::tbl(duckCon, "duckbase") |>
    dplyr::group_by(start_station_id) |>
    dplyr::summarize(StationCount = count()) |>
    dplyr::arrange(StationCount, start_station_id)
```

With DuckDB, approx 0.11s to return query results.

```{r}
dataquet |>
    dplyr::collect() |>
    dplyr::distinct(ride_id) |>
    dplyr::count(name = "Distinct Ride ID's")
```

In memory parquet file query took around 2.55s. Personal observation, increased memory usage by about 1GB for non-DuckDB queries and around 0.2GB for DuckDB (DDB) queries. Clear advantage goes to: DDB.

Running SQL syntax to the DDB has the same results as dplyr package syntax. These are relatively faster and more memory efficient than querying an in memory dataframe.

```{sql, connection = duckCon, output.var = "Distinct"}

SELECT COUNT 
    (DISTINCT ride_id) AS "Distinct Obs"
FROM
    duckbase
```

```{r}
Distinct
```

```{sql, connection = duckCon, output.var = "MonthlyRiders"}

SELECT MonthNum, AVG(COUNT) AS RidesPerMonth
FROM (
    SELECT duckbase.*,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
)
GROUP BY MonthNum
ORDER BY RidesPerMonth
```

```{sql, connection = duckCon, output.var = "MonthlyAVG"}

SELECT AVG(RidesPerMonth) AS "MonthlyAVG"
FROM (
  SELECT MonthNum, COUNT() AS RidesPerMonth
  FROM (
    SELECT
      duckbase.*,
      EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
  )
  GROUP BY MonthNum
)
```

```{r}

newConnection <- dplyr::tbl(duckCon, "duckbase") |>
    dplyr::select(started_at, member_casual) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "casual") |>
    dplyr::summarize(Casuals = count(member_casual))


newConnection2 <- dplyr::tbl(duckCon, "duckbase") |>
    dplyr::select(started_at, member_casual) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "member") |>
    dplyr::summarize(Members = count(member_casual))

    
    # dplyr::group_by(member_casual) |>
    #dplyr::summarize(Membership = count())

    #dplyr::group_by(MonthNum = lubridate::month(
    #    lubridate::as_datetime(started_at))) |>

newConnection2
```

```{r}
newConnection
```

```{r}
groupCount_month <- dplyr::tbl(duckCon, "duckbase") |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::summarize(RidesPerMonth = count()) |>
    dplyr::summarize(mean(RidesPerMonth))
```

```{r}
groupCount_month <- dplyr::tbl(duckCon, "duckbase") |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::summarize(RidesPerMonth = count()) |>
    dplyr::arrange(RidesPerMonth) |>
    dplyr::show_query()
```

```{sql, connection = duckCon, output.var = "what"}

SELECT month, COUNT(member_casual)
FROM duckbase
GROUP BY month
ORDER BY month
```

```{r}
what
```

```{r}

```

# Descriptive Statistics

#### Stats: Monthly Average

```{sql, connection = duckCon, output.var = "MonthlyAVG"}

SELECT AVG(RidesPerMonth) AS "MonthlyAVG"
FROM (
  SELECT MonthNum, COUNT() AS RidesPerMonth
  FROM (
    SELECT
      duckbase.*,
      EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
  )
  GROUP BY MonthNum
```

```{r}
cat(stringr::str_c(as.integer(MonthlyAVG)))
```
