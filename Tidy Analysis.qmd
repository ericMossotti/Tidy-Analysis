---
title: "Professional Data Analysis"
author: "Eric Mossotti"

code-fold: true

df_print: "tibble" 

format: html
toc: true
---

```{r include = FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)
```

# Overview

This project was built from the ground up. I wanted to reach a broad audience with what I consider as a major data analysis project. I wanted to redo the Google Data Analytics capstone project to better my chances of reaching a larger audience. Also, the dataset is immense with millions of observations from just a single year. It seems to be regularly updated with more data, interestingly enough, despite being simulated.

I finished the course years ago and did some minimal analysis to meet the course requirements, but didn't do as much as I wanted. I reused some of the course materials as helpful, general purpose reminders. The dataset doesn't require too much domain knowledge to understand but is realistic (with errors) and challenging enough to be worthy of processing and analysis.

## Importing Data Efficiently

I think .ZIP files would be commonplace as they substantially reduce long term storage requirements. Here, I wrote a short function that downloads a set of zip files from web URLs which then extracts the files of interest to a separate folder.

```{r}

# Efficiently creates a list of download addresses. 
durls <-
    sprintf("https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip",
            202301:202312)

zipPaths <- sprintf("tripZips/%d-divvy-tripdata.zip",
                     202301:202312)

# Define file paths for .CSVs. 
filePaths <- sprintf("tripdata/%d-divvy-tripdata.csv",
                      202301:202312)

# Define filenames for the .ZIP files. 
zipNames <- sprintf("%d-divvy-tripdata.zip",
                     202301:202312)

# Define filenames for .CSV's
fileNames <- sprintf("%d-divvy-tripdata.csv",
                      202301:202312)

pq_path <- "pq_tripdata"

# I need some directories to store the files. 
dir.create("tripZips")

dir.create("tripdata")

dir.create("pq_tripdata")
```

### File Downloads

```{r}

# Need to download several compressed files from URLs to the directory destination and filepaths (zipPaths) previously created.
curl::multi_download (durls,
                      destfiles = zipPaths)
```

### Extracting .CSV from .ZIP

```{r}

# A custom function that makes unzipping and moving files easy and fun.
unz_csv <- function (x = filePaths,
                     y = zipPaths,
                     z = fileNames) {
    
    for (i in seq(x)) {
        utils::unzip (y[i],
                      z[i])
        
        file.rename (z[i],
                     x[i])
    }
}
```

```{r}

# Wanted to verify where the extracted data is at. 
csvFileList <- list.files(path = "tripdata",
                          full.names = TRUE)

matrix(csvFileList,
       dimnames = list(seq(csvFileList),
                       c("csvfiles")))
```

```{r}

# Establish a new connection to a virtual in memory DuckDB database
dukonn <- DBI::dbConnect(duckdb::duckdb())

# Register the dataset connection object to a location in the DDB
duckdb::duckdb_register_arrow(duckCon,
                              "duckbase",
                              dataquet)

# Saves memory by avoiding loading all of the data into memory as a dataframe? Dataset that points towards a CSV file system (can it be considered "partitioned"?)

# Make dataset more memory efficient dataset for analysis.
arrow::open_dataset(sources = "tripdata",
                               format = "csv") |>
    dplyr::group_by(Month = lubridate::month(started_at)) |>
    arrow::write_dataset(path = "tripdata-pqfs",
                         format = "parquet")

tripset <- arrow::open_dataset(sources = "tripdata-pqfs",
                               format = "parquet")

dbconn <- DBI::dbConnect(duckdb::duckdb())

duckdb::duckdb_register_arrow(dbconn,
                              "tripdb",
                              tripset)
```

```{r}

# With list_rbind(), map can be used to create a dataframe from a list of files
#oneTibble <- purrr::map(csvFileList[1:12], 
 #                       read_csv) |>
#   purrr::list_rbind ()
```

```{r}

oneTibble[1,]
```

```{r}

length(oneTibble[[1]])
```

We started with 5,719,877 observations for the time period of Jan-Dec 2023, then drop_na() removed 1,388,170 of those observations.

```{r}

# Pretty straightforward way to remove incomplete observations.
oneTibble <- tidyr::drop_na(oneTibble)
```

```{r}

# Keeping track of observations/rows dropped.
tibble::tibble(
    Complete_Observations = length(oneTibble [[1]]),
    "Rows Deleted by drop_na()" = originalLength - Complete_Observations
)
```

```{r}

# Maybe only need to check ride_id column? I wish.
dplyr::distinct(oneTibble, dplyr::pick ("ride_id")) |>
    dplyr::count(name = "Distinct Ride ID's")
```

Initially, the assumption that 4,331,707 is what should be expected as total unique observations. Perhaps it's not that easy, though? Checking the other possible parameters might be worth the time.

Of the other columns, it seems that the start_time, end_time, start_station, and end_station could show if there are hidden duplicated observations. There is no mention that more than one bike can be checked out at once.

I have assumed that due the logistics of having the exact same times/dates and stations for two different riders is impossible. Another option would be to assume one person could check out multiple cycles at once. But it has only happened 18 times over the course of a full year. I would think that would happen more often because people probably ride in groups often. A scenario where one person pays and gets "*Venmo'd*" later by their friends comes to mind.

```{r}

# This is a separate table used to analyze the observations returned as not distinct (n > 1). This adds an extra column labeled "n" because I didn't want to add more complexity to the main dataframe.
dupTibble <- oneTibble |>
    dplyr::select(started_at:end_station_name) |>
    # Counts of rows for column 'n'
    dplyr::add_count(started_at,
                     ended_at,
                     start_station_name,
                     end_station_name) |>
    # Only observations that have been duplicated 1 or more 
    # times are shown
    dplyr::filter(n > 1) |>
    # We want to see all rows, not just one row for each obs
    dplyr::ungroup() |>
    dplyr::arrange(started_at)
```

```{r}

print(dupTibble)

paste0("Number of observations with duplicates: ",
       length(dupTibble[[1]]))

dplyr::distinct(dupTibble["n"])
```

By applying distinct() on dupTibble, we see the only distinct value is 2. We can safely conclude that of the duplicates each duplicated observation has only 1 extra copy.

```{r}

# verify how many rows have/are duplicates
nrow(dupTibble)
```

Number of rows in the dupTibble is 36. Because each duplicated observation has one duplicate (n = 2), expected removed nobs is 18.

```{r}

# The issue is, we need to get rid of not all of these rows, but just the extra duplicate observations. 

# If there were 2 rows of duplicates, we would want to end up with 1 row after removing the extras.
undupdTibble <- dupTibble |>
    dplyr::distinct(started_at,
                     start_station_name,
                     ended_at,
                     end_station_name,
                     .keep_all = TRUE)
```

```{r}

print(undupdTibble)
```

```{r}

nrow(undupdTibble)
```

Our predicted value for observations in the undupdTibble was indeed 18.

```{r}

incorrectDistinct <-
    dplyr::distinct(oneTibble, dplyr::pick("ride_id")) |>
    dplyr::count(name = "Incorrect Distinct Observations")

incorrectDistinct
```

```{r}

correctDistinct <-
    dplyr::distinct(
        oneTibble,
        dplyr::pick(
            "started_at",
            "start_station_name",
            "ended_at",
            "end_station_name"
        )
    ) |>
    dplyr::count(name = "Corrected Distinct Observations")

correctDistinct
```

```{r}

correctionTibble <-
    tibble::tibble(
        Uncorrected = uncorrectDistinct[[1]],
        Expected_Corrected = correctDistinct[[1]],
        Removed_Obs = Uncorrected - Corrected
    )

correctionTibble
```

Uncorrected number of observations (nobs) after drop_na() was 4,331,707. The corrected nobs after removing one copy of the duplicates since all of them had only one duplicate, resulted in 4,331,689 nobs. By applying distinct() on the tibble for 4 parameters at once, 18 additional observations were removed.

```{r}

oneTibble <- oneTibble |>
    dplyr::distinct(started_at, 
                    start_station_name, 
                    ended_at,
                    end_station_name, 
                    .keep_all = TRUE)
```

```{r}

nrow(oneTibble)
```

```{r}

# sorting the entire table by the start_at column
oneTibble <- oneTibble |>
    dplyr::arrange(started_at)
```

It doesn't really seem like the station IDs containing decimals (e.g., ####.# or ####.#.#) were typos. I tried asking if station IDs containing decimal points are meant to be that way. There were no station ids with decimal numbers other than 0 or 1 after the decimal place. There were many different station IDs with one 0 after the decimal. There was only 1 station ID with a 1 following the decimal and it was in the form of "###.1.1".

# Efficiency Side-Quest

## Parquet Filesystem

The multi-file partitioning of the dataset should result in faster queries in certain situations. Parquet files require less long term storage space than CSV's as well.

[R for Data Science: Chapter 22: Arrow](https://r4ds.hadley.nz/arrow "Arrow")

### Comparing .Parquet and .CSV Performance

```{r}

# Move tibble out of memory and into a long term storage file(s). 
arrow::write_parquet(oneTibble, sink = "oneQuet")

# Create a dataset.
dataquet <- arrow::open_dataset("oneQuet",
                                format = "parquet")

# Create partitioned dataset.
dataquet <- dataquet |>
    dplyr::group_by(month = lubridate::month(started_at))

# Write a partitioned backup filesystem so we don't have to do any of the above steps again moving forward. 
dataquet |>
    arrow::write_dataset(path = "parquets",
                         format = "parquet")

# Dataset needs to point towards the new partitioned parquet file system.
dataquet <- arrow::open_dataset(sources = "parquets",
                              format = "parquet")

# Dataset that points towards our original CSV file system (can it be considered "partitioned"?)  
csvset <- arrow::open_dataset(sources = "tripdata",
                              format = "csv")

# I also wanted to test performance of dataset based on "un-partitioned" .CSV file system
onecsvset <- csvset |>
    arrow::write_csv_arrow(sink = "oneCSVfile")

# Arrow dataset needs to point to the .CSV file
onecsvset <- arrow::open_csv_dataset(sources = "oneCSVfile")
```

We test the performance of a query to a one .CSV file dataset.

```{r}

onecsvset |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

Testing the performance of a query to a "partitioned" .CSV file dataset.

```{r}

csvset |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

Testing the performance of a query to a partitioned .Parquet file dataset.

```{r}

dataquet |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

```{r}

# Need do some more testing. Opening from .Parquet file made previously. 
twoTibble <- arrow::read_parquet("oneQuet", as_data_frame = TRUE)

# Having file backups of pre-processed data comes in handy. 
oneTibble <- readr::read_csv("oneCSVfile")

twoTibble["end_station_id"] |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::count()

#
twoTibble["end_station_id"] |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::count()

#
twoTibble["end_station_id"] |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "[1][0][7][9]")) |>
    dplyr::count()

##
oneTibble["end_station_id"] |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::count()

#
twoTibble["end_station_id"] |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[1-9]$")) |>
    dplyr::arrange(end_station_id)

#
twoTibble["end_station_id"] |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::mutate(
        end_station_id = stringr::str_replace(
            string = end_station_id,
            pattern = "\\.[:digit:]$",
            replacement = ""
        )
    )
```

## DuckDB: In memory OLAP Database

[Why DuckDB?](https://duckdb.org/why_duckdb)

You can read in the above link why I would choose to use DuckDB and OLAP for this project.

```{r}

# Open .Parquet file dataset connection pointer
dataquet <- arrow::open_dataset(sources = "parquets",
                                format = "parquet")
# Establish a new connection to a virtual in memory DuckDB database
duckCon <- DBI::dbConnect(duckdb::duckdb())

# Register the dataset connection object to a location in the DDB
duckdb::duckdb_register_arrow(duckCon,
                              "duckbase",
                              dataquet)
```

```{r}

# Confirm table info
dplyr::tbl(duckCon, "duckbase") |>
         dplyr::glimpse()
```

## SQL Queries

```{r}

sql_countRiders <- paste0(
    "SELECT COUNT(*) AS 'Distinct Riders' ",
    "FROM (",
    "SELECT DISTINCT ride_id ",
    "FROM duckbase)"
)

qres_countRiders <- duckdb::dbSendQuery(conn = duckCon,
                                        statement = sql_countRiders)

fetch_countRiders <- duckdb::dbFetch(qres_countRiders)

fetch_countRiders

duckdb::dbClearResult(qres_countRiders)
```

```{r}

# Makes formatting simpler for SQL syntax
sql_selectHead <- paste0("SELECT(*) ",
                         "FROM duckbase ",
                         "LIMIT 10")

# Query result variable
qres_selectHead <- duckdb::dbSendQuery(conn = duckCon,
                                       sql_selectHead)
# Results of query in the form of a dataframe
duckdb::dbFetch(qres_selectHead)
# To save on overall memory overhead
duckdb::dbClearResult(qres_selectHead)
```

```{r}

rCounts_types <- dplyr::tbl(duckCon, "duckbase") |>
    dplyr::group_by(rideable_type) |>
    dplyr::summarize(rideTypecount = count(), .groups = "keep")

groupCount_rideTypes
#or this
sql_ridetypeCounts <- paste0(
    "SELECT rideable_type, count() AS 'rt_counts' ",
    "FROM duckbase ",
    "GROUP BY rideable_type"
)

qres_ridetypeCounts <- duckdb::dbSendQuery(conn = duckCon,
                                           statement = sql_ridetypeCounts
 )

fres_rideTypeCounts <- duckdb::dbFetch(qres_ridetypeCounts)
#duckdb::dbClearResult(qres_ridetypeCounts)
```

```{r}

fres_rideTypeCounts
dplyr::glimpse(fres_rideTypeCounts)

newPlot <- ggplot2::ggplot(data = fres_rideTypeCounts,
                           mapping = ggplot2::aes(
                               x = rideable_type,
                               y = rt_counts)) +
    ggplot2::geom_col(fill = "red")

newPlot
```

```{r}

dplyr::tbl(duckCon, "duckbase") |>
    dplyr::group_by(start_station_id) |>
    dplyr::summarize(StationCount = count()) |>
    dplyr::arrange(StationCount, start_station_id)
```

With DuckDB, approx 0.11s to return query results.

```{r}

dataquet |>
    dplyr::collect() |>
    dplyr::distinct(ride_id) |>
    dplyr::count(name = "Distinct Ride ID's")
```

In memory parquet file query took around 2.55s. Personal observation, increased memory usage by about 1GB for non-DuckDB queries and around 0.2GB for DuckDB (DDB) queries. Clear advantage goes to: DDB.

Running SQL syntax to the DDB has the same results as dplyr package syntax. These are relatively faster and more memory efficient than querying an in memory dataframe.

```{sql, connection = duckCon, output.var = "Distinct"}

SELECT COUNT 
    (DISTINCT ride_id) AS "Distinct Obs"
FROM
    duckbase
```

```{sql, connection = duckCon, output.var = "MonthlyRiders"}

SELECT MonthNum, AVG(COUNT) AS RidesPerMonth
FROM (
    SELECT duckbase.*,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
)
GROUP BY MonthNum
ORDER BY RidesPerMonth
```

```{sql, connection = duckCon, output.var = "MonthlyAVG"}

SELECT AVG(RidesPerMonth) AS "MonthlyAVG"
FROM (
  SELECT MonthNum, COUNT() AS RidesPerMonth
  FROM (
    SELECT
      duckbase.*,
      EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
  )
  GROUP BY MonthNum
)
```

```{sql, connection = duckCon, output.var = "sql_counts_month_rides"}

SELECT month, COUNT(member_casual) AS Rider_count
FROM duckbase
GROUP BY month
ORDER BY month
```

# Descriptive Statistics

#### Stats: Monthly Average

```{sql, connection = duckCon, output.var = "sql_avg_month_rides"}

SELECT AVG(RidesPerMonth) AS "MonthlyAVG"
FROM (
  SELECT MonthNum, COUNT() AS RidesPerMonth
  FROM (
    SELECT
      duckbase.*,
      EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
  )
  GROUP BY MonthNum
)
```

```{r}

cat(stringr::str_c(as.integer(MonthlyAVG)))
```

```{sql, connection = duckCon, output.var = "sql_counts_month_casual"}

SELECT MonthNum, COUNT(member_casual) AS Casuals
FROM (
  SELECT
    started_at,
    member_casual,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
  FROM duckbase
  WHERE (member_casual = 'casual')
)
GROUP BY MonthNum
```

#### Average Casuals Per Month

```{sql, connection = duckCon, output.var = "sql_AVG_casual"}

SELECT AVG(Casuals) AS AvgCasuals
FROM (
  SELECT MonthNum, COUNT() AS Casuals
  FROM (
    SELECT
      started_at,
      member_casual,
      EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
    FROM duckbase
    WHERE (member_casual = 'casual')
  )
  GROUP BY MonthNum
)
```

```{r}

cat(stringr::str_c(as.integer(AVG_casuals_monthly)))
```

# testing section

```{sql, connection = duckCon, output.var = "sql_counts_month_casual_e"}

SELECT MonthNum, COUNT(member_casual) AS CasualsOnElectric
FROM (
  SELECT
    started_at,
    member_casual,
    rideable_type,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
  FROM duckbase
  WHERE (member_casual = 'casual') AND (rideable_type = 'electric_bike')
)
GROUP BY MonthNum
```

```{sql, connection = duckCon}

CREATE TABLE tbl_eMembers AS
SELECT MonthNum, COUNT(member_casual) AS MembersOnElectric
FROM (
  SELECT
    started_at,
    member_casual,
    rideable_type,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
  FROM duckbase
  WHERE (member_casual = 'member') AND (rideable_type = 'electric_bike')
)
GROUP BY MonthNum
```

```{sql, connection = duckCon}

CREATE TABLE tbl_eCasuals AS 
SELECT MonthNum, COUNT(member_casual) AS CasualsOnElectric
FROM (
  SELECT
    started_at,
    member_casual,
    rideable_type,
    EXTRACT(MONTH FROM CAST(started_at AS TIMESTAMP)) AS MonthNum
  FROM duckbase
  WHERE (member_casual = 'casual') AND (rideable_type = 'electric_bike')
)
GROUP BY MonthNum
```

```{r}

duckdb::duckdb_list_arrow (duckCon)
    
duckdb::dbListTables(duckCon)
    
dplyr::tbl(duckCon, "tbl_eCasuals") |>
        dplyr::select_all()
    
dplyr::tbl(duckCon, "tbl_eMembers") |>
    dplyr::select_all()
```

```{sql, connection = duckCon}

SELECT tbl_eMembers.*, CasualsOnElectric
FROM tbl_eMembers
LEFT JOIN tbl_eCasuals
  ON (tbl_eMembers.MonthNum = tbl_eCasuals.MonthNum)
```

### One Big Table?

```{r}

duckdb::duckdb_list_arrow(duckCon)

rAvg_riders <-
    dplyr::tbl(duckCon,
               "duckbase") |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::summarize(RidesPerMonth = count()) |>
    dplyr::summarize(AvgRiders = mean(RidesPerMonth))

rAvg_casuals <-
    dplyr::tbl(duckCon,
               "duckbase") |>
    dplyr::select(started_at, member_casual) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "casual") |>
    dplyr::summarize(Casuals = count()) |>
    dplyr::summarize(AvgCasuals = mean(Casuals))


rCounts_types

rAvg_riders
rAvg_casuals
```

##### Counts_By_Month

```{r}

# Create a monthly count summaries table to be joined by other tables
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::summarize(RidesPerMonth = count()) |>
    dplyr::arrange(MonthNum) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month")
```

Verifying the new table and data

```{r}

dplyr::tbl(duckCon, "Counts_By_Month") |>
    tibble::view(n = 12)
```

Adding a new column by joining a dataframe object to the Counts_By_Month table by month.

```{r}

# monthly casual counts
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "casual") |>
    dplyr::summarize(Casuals = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month_Casuals")
```

```{r}

dplyr::tbl(duckCon, "Counts_By_Month::Casual Riders") |>
    tibble::view(n = 12)
```

```{r}

dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at, member_casual) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "member") |>
    dplyr::summarize(Annuals = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month_Annuals")
```

```{r}

dplyr::tbl(duckCon, "Counts_By_Month::Annual Members") |>
    tibble::view(n = 12)
```

```{r}

duckdb::dbListTables(duckCon)
```

```{r}

# create a monthly casual ebike riders table
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual,
                  rideable_type) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "casual",
                  rideable_type == "electric_bike") |>
    dplyr::summarize(Casual_eBikers = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month_Casual_eBikers")
```

```{r}

dplyr::tbl(duckCon, "Counts_By_Month::Casual eBikers") |>
    tibble::view(n = 12)
```

```{r}

# create a monthly annual member ebike riders table
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual,
                  rideable_type) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "member",
                  rideable_type == "electric_bike") |>
    dplyr::summarize(Annual_eBikers = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = "Counts_By_Month_Annual_eBikers")
```

```{r}

dplyr::tbl(duckCon, "Counts_By_Month::Annual Member eBikers") |>
    tibble::view(n = 12)
```

```{r}

# create a monthly annual member classic riders table
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual,
                  rideable_type) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "member",
                  rideable_type == "classic_bike") |>
    dplyr::summarize(Annual_classicBikers = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = 
                             "Counts_By_Month_Annual_classicBikers")
```

```{r}

dplyr::tbl(duckCon, 
           "Counts_By_Month::Annual Members::Classic Bikers") |>
    tibble::view(n = 12)
```

```{r}

# create a table of monthly counts for casual riders on classic bikes
dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::select(started_at,
                  member_casual,
                  rideable_type) |>
    dplyr::group_by(MonthNum = lubridate::month(
        lubridate::as_datetime(started_at))) |>
    dplyr::filter(member_casual == "casual",
                  rideable_type == "classic_bike") |>
    dplyr::summarize(casual_classicBikers = count(member_casual)) |>
    as.data.frame() |>
    duckdb::dbWriteTable(conn = duckCon,
                         name = 
                             "Counts_By_Month_Casual_classicBikers")
```

```{r}

dplyr::tbl(duckCon, 
           "Counts_By_Month::Casuals::Classic Bikers") |>
    tibble::view(n = 12)
```

```{r}
duckdb::dbListTables(duckCon)
```

```{r}

dplyr::tbl(duckCon,
           "Counts_By_Month") |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Casuals")) |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Annuals")) |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Casual_eBikers")) |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Annual_eBikers")) |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Casual_classicBikers"))  |>
    dplyr::left_join(y = dplyr::tbl(duckCon,
                                    "Counts_By_Month_Annual_classicBikers"))
```

```{r}


dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::group_by(rideable_type) |>
    dplyr::summarize(rideTypecount = count(),
                     .groups = "keep")
```
