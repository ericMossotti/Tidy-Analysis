---
title: "Tidy Analysis"
author: "Eric Mossotti"

code-fold: true

df_print: "tibble" 

format: html
toc: true
---

```{r include= FALSE}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# Data Cleaning: The Heavy Lifting

```{r}

durls <- sprintf("https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip",
                  202301:202312)

zipPaths <- sprintf("tripZips/%d-divvy-tripdata.zip",
                     202301:202312)

filePaths <- sprintf("tripdata/%d-divvy-tripdata.csv",
                      202301:202312)

zipNames <- sprintf("%d-divvy-tripdata.zip",
                     202301:202312)

fileNames <- sprintf("%d-divvy-tripdata.csv",
                      202301:202312)

pq_path <- "pq_tripdata"

dir.create("tripZips")

dir.create("tripdata")

dir.create ("pq_tripdata")

curl::multi_download (durls,
                      destfiles = zipPaths)

unz_csv <- function (x = filePaths,
                     y = zipPaths,
                     z = fileNames) {
    for (i in seq(x)) {
        utils::unzip (y[i],
                      z[i])
        
        file.rename (z[i],
                     x[i])
    }
}
```

```{r}

csvFileList <- list.files (path = "tripdata",
                           full.names = TRUE)

# view {tibble}
matrix (csvFileList,
        dimnames = list (seq (csvFileList),
                         c ("csvfiles")))
```

```{r}

# map {purrr}, 

# With list_rbind(), map can be used to directly create a dataframe from a list of files
oneTibble <- purrr::map (csvFileList[1:12], read_csv) |>
    
    purrr::list_rbind ()
```

( In R a tibble is synonymous with a data frame )

```{r}

typeof (oneTibble)

is_tibble (oneTibble)

is.data.frame (oneTibble)
```

```{r}
oneTibble[1,]
```

```{r}
originalLength <- length (oneTibble[[1]])

print(originalLength)
```

We started with 5,719,877 observations for the time period of Jan-Dec 2023, then drop_na() removed 1,388,170 of those observations.

```{r}

# drop_na {tidyr}	
oneTibble <- tidyr::drop_na (oneTibble)
```

```{r}
tibble::tibble (
    Complete_Observations = length (oneTibble [[1]]),
    "Rows Deleted by drop_na()" = originalLength - Complete_Observations
)
```

```{r}

# Maybe only need to check ride_id column? Actually, no.

# pick {dplyr}	
dplyr::distinct (oneTibble, dplyr::pick ("ride_id")) |>
    
    dplyr::count (name = "Distinct Ride ID's")
```

Initially, the assumption that 4,331,707 is what should be expected as total unique observations. Perhaps it's not that easy, though? Checking the other possible parameters might be worth the time.

Of the other columns, it seems that the start_time, end_time, start_station, and end_station could show if there are hidden duplicated observations. There is no mention that more than one bike can be checked out at once. One big limitation of this analysis is that it's based on a fictional but realistic dataset.

For the sake of this exercise it's assumed that due the logistics of having the exact same times/dates and stations for two different riders is impossible. The other option would be to assume one person could check out and in multiple cycles. It's only happened 18 times over the course of a full year. In the real world, we would want to be absolutely sure, but for this exercise, it is reasonable to assume these are errors.

```{r}

# This is a separate table used to analyze the observations returned as not distinct (n > 1). This adds an extra column (n) so didn't want to mess with the main dataframe for this.

dupTibble <- oneTibble |>
    
    dplyr::select (started_at:end_station_name) |>
    
    # adds a count of rows for column 'n'
    dplyr::add_count(started_at,
                      ended_at,
                      start_station_name,
                      end_station_name) |>
    
    # so that only observations that have been duplicated 1 or more         times are shown
    dplyr::filter(n > 1) |>
    
    # because we want to see all the rows, not just one row for each obs
    dplyr::ungroup() |>
    
    dplyr::arrange(started_at)
```

```{r}

print (dupTibble)


paste0("Number of observations with duplicates: ",
       length(dupTibble[[1]]))


dplyr::distinct (dupTibble["n"])
```

By applying distinct() on dupTibble, we see the only distinct value is 2. We can safely conclude that of the duplicates each duplicated observation has only 1 extra copy.

```{r}

# verify how many rows have/are duplicates
nrow (dupTibble)
```

Number of rows in the dupTibble is 36. Because each duplicated observation has one duplicate (n = 2), expected removed nobs is 18.

```{r}

# issue is, we need to get rid of not all of these rows, but just the extra duplicate observations. 

# If there were 2 rows of duplicates, we would want to end up with 1 row after removing the extras.

undupdTibble <- dupTibble |>
    
    dplyr::distinct (started_at,
                     start_station_name,
                     ended_at,
                     end_station_name,
                     .keep_all = TRUE)
```

```{r}

print (undupdTibble)
```

```{r}

nrow (undupdTibble)
```

Our predicted value for observations in the undupdTibble was indeed 18.

```{r}

uncorrectDistinct <-
    dplyr::distinct (oneTibble, dplyr::pick ("ride_id")) |>
    
    dplyr::count (name = "Uncorrected Distinct Observations")
uncorrectDistinct
```

```{r}

correctDistinct <-
    dplyr::distinct (
        oneTibble,
        dplyr::pick (
            "started_at",
            "start_station_name",
            "ended_at",
            "end_station_name"
        )
    ) |>
    
    dplyr::count (name = "Corrected Distinct Observations")

correctDistinct

correctionTibble <-
    tibble::tibble(
        Uncorrected = uncorrectDistinct[[1]],
        Expected_Corrected = correctDistinct[[1]],
        Removed_Obs = Uncorrected - Corrected
    )

correctionTibble
```

Uncorrected number of observations (nobs) after drop_na() was 4,331,707. The corrected nobs after removing one copy of the duplicates since all of them had only one duplicate, resulted in 4,331,689 nobs. By applying distinct() on the tibble for 4 parameters at once, 18 observations were removed.

```{r}

oneTibble <- oneTibble |>
    
    dplyr::distinct (started_at, 
                    start_station_name, 
                    ended_at,
                    end_station_name, 
                    .keep_all = TRUE )
```

```{r}
nrow(oneTibble)
```

```{r}
oneTibble[3] |>
    
    dplyr::arrange ( started_at ) |>
    
    head ()
```

```{r}
# sorting the entire table by the start_at column
oneTibble <- oneTibble |>
    dplyr::arrange (started_at)
```

```{r}
print(head(oneTibble[3]))
```

```{r}

# Split cells
oneTibble[3] |>
    
    # separate_wider_delim {tidyr}
    tidyr::separate_wider_delim (
        cols = c (started_at),
        delim = " ",
        names = c ("start_date",
                   "start_time"),
        cols_remove = TRUE
    ) |>
    
    head()
```

```{r}

oneTibble[4] |>
    
    tidyr::separate_wider_delim (
        cols = c(ended_at),
        delim = " ",
        names = c("end_date",
                  "end_time"),
        cols_remove = TRUE
    ) |>
    
    head()
```

Changing the data type of the date columns.

```{r}

#test
oneTibble |>
    
    dplyr::mutate (
        start_date = lubridate::as_datetime(start_date,
                             format = "%Y-%m-%d"),
        end_date = lubridate::as_datetime(end_date,
                           format = "%Y-%m-%d")
    ) 

# changes date columns to the date dataype columns
oneTibble <- oneTibble |>
    
    dplyr::mutate(
        start_date = lubridate::as_date(start_date,
                             format = "%Y-%m-%d"),
        end_date = lubridate::as_date(end_date,
                           format = "%Y-%m-%d")
    )

#test
oneTibble |>
    dplyr::mutate (start_time = hms::as_hms(start_time),
                   end_time = hms::as_hms(end_time))
    
oneTibble <- oneTibble |>
    dplyr::mutate (start_time = hms::as_hms(start_time),
                  end_time = hms::as_hms(end_time))
```

```{r}
oneTibble[3] |> head()

oneTibble[3] |>
    tidyr::separate_wider_delim (
        cols = c(start_date),
        delim = "-",
        names = c("Start Year",
                  "Start Month",
                  "Start Day"),
        cols_remove = FALSE
    ) |>
    head()
```

```{r}
# Split cells
oneTibble <- oneTibble |>
    
    # separate_wider_delim {tidyr}
    tidyr::separate_wider_delim (
        cols = c (started_at),
        delim = " ",
        names = c ("start_date", "start_time"),
        cols_remove = TRUE
    )
```

```{r}
tibble_row ("How many columns?" = ncol (oneTibble))
```

Now there are 14 columns total.

```{r}

oneTibble <- oneTibble |>
    
    tidyr::separate_wider_delim (cols = c(ended_at),
                                 delim = " ",
                                 names = c("end_date",
                                           "end_time"),
                                 cols_remove = TRUE )
```

```{r}

tibble::tibble_row("How many columns?" = ncol (oneTibble))
```

Â Now there are 15 total columns.

```{r}

year(as_datetime(oneTibble[[3]][1:4]))


```

```{r}
oneTibble |>
    
    dplyr::group_by(lubridate::month(oneTibble[[started_at]]))
```

# Data Prep: Lighter Lifting with Apache Arrow

## Created Parquet Filesystem

The multi-file partioning of the dataset should result in faster queries. Parquet files require less disc space as well.

```{r}

# from CSV's, now a single dataframe, to partioned parquet files?


# Just needed the tidied tibble out of memory and into an actual file. 
arrow::write_parquet(oneTibble, sink = "oneQuet")

# 
dataquet <- arrow::open_dataset("oneQuet",
                                format = "parquet")

dataquet <- dataquet |>
    dplyr::group_by(month = lubridate::month(started_at))

dataquet |>
    arrow::write_dataset(path = "parquets",
                         format = "parquet")
    
dataquet <- arrow::open_dataset(sources = "parquets",
                              format = "parquet")

csvset <- arrow::open_dataset(sources = "tripdata",
                              format = "csv")

onecsvset <- csvset |>
    arrow::write_csv_arrow(sink = "oneCSVfile")

onecsvset <- arrow::open_csv_dataset(sources = "oneCSVfile")

```

```{r}

onecsvset |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

```{r}

csvset |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

```{r}
dataquet |>
    dplyr::filter(rideable_type == "classic_bike") |>
    group_by(month) |>
    collect() |>
    system.time()
```

## Arrow and DuckDB

[R for Data Science: Chapter 22: Arrow](https://r4ds.hadley.nz/arrow "Arrow")

```{r}

duckCon <- DBI::dbConnect(duckdb::duckdb ())

duckdb::duckdb_register_arrow(duckCon,
                              "duckbase",
                              dataquet)

dplyr::tbl(duckCon,
           "duckbase") |>
    dplyr::distinct(pick(ride_id)) |>
    dplyr::count(name = "Distinct") |>
    system.time()
```

```{r}
dataquet |>
    dplyr::collect() |>
    dplyr::distinct(dplyr::pick(ride_id)) |>
    dplyr::count(name = "Distinct Ride ID's") |>
    system.time()
```
