---
title: "Tidy Analysis"
author: "Eric Mossotti"

code-fold: true

df_print: "tibble" 

format: html
toc: true
---

```{r include= FALSE}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)

```

# Project Environment Set Up

## 

*tidyverse-package {tidyverse}:*

> This package is designed to make it easy to install and load multiple 'tidyverse' packages in a single step.

```{r}

library ( tidyverse )
```

## Data Import

### Sourcing

```{r}

library(dplyr)

# "https://divvy-tripdata.s3.amazonaws.com/202004-divvy-tripdata.zip"
# "202004


#con <- DBI::dbConnect(RSQLite::SQLite(), dbname = ":memory:")

durls <- sprintf ( 
    "https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip",
    202301:202312 )

zipNames <- sprintf ( "%d-divvy-tripdata.zip",
                      202301:202312 )
#tempfile {base}
tempZips <- tempfile ( zipNames )

# multi_download {curl}	
multi_download ( durls, destfiles = tempZips )

fileNames <- sprintf ( "%d-divvy-tripdata.csv",
                   202301:202312)
```

```{r}
unz_read <- function ( fileNames, tempZips ) {
    
    datalist <- list()
    
    for ( i in 1:length(fileNames) ) {
        
            csvFile <- unz (tempZips[i], 
                         fileNames[i] )
            
            datalist[[i]] <- data.frame(read.csv(csvFile))
            #newdf <- read.csv ( newTempfile )
        
    }
    
    return ( datalist )
}
```

```{r}

oneTibble <- do.call ( rbind, 
                       unz_read ( fileNames = fileNames, 
                                  temp = temp )
                       )

```

## More Efficient Data Import with Arrow

[R for Data Science: Chapter 22: Arrow](https://r4ds.hadley.nz/arrow "Arrow")

```{r}
durls <- sprintf ( 
    "https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip",
    202301:202312 )

dir.create("tripZips")

zipPaths <- sprintf ( "tripZips/%d-divvy-tripdata.zip",
                      202301:202312 )

multi_download ( durls, destfiles = zipPaths )

dir.create("tripdata")

filePaths <- sprintf ( "tripdata/%d-divvy-tripdata.csv",
                             202301:202312)

for ( i in 1:12 ) {
    
 }



library(arrow)

# open_dataset {arrow}	
trip_dataset <- open_dataset("tripdata", format = "csv")


glimpse(trip_dataset)


trip_dataset |>
    
    summarise (.by = rideable_type, n = n() ) |>
    
    collect()|>

    system.time()

```

```{r}

trip_dataset <- open_dataset(
    
    sources = "tripdata",
    
    format = "csv"
)

dir.create("tripdata/ride_types")
pq_path <- "tripdata/ride_types"

trip_dataset |>
    
    group_by(rideable_type)|>
    
    write_dataset(path = pq_path, 
                  format = "parquet")


tibble(
  files = list.files(pq_path, recursive = TRUE),
  size_MB = file.size(file.path(pq_path, files)) / 1024^2
)


rideType_pq <- open_dataset(pq_path)

rideType_pq |>
    collect()|>
    distinct ( pick(ride_id))|>
    count(name = "Distinct obs")|>
    system.time()
```

I am not sure if I should keep this all in one file or try to split files based on potential grouping before, after, or during cleaning.

Let's try to partition it out, then see how the performance differs.

```{r}

rideType_tidy <- rideType_pq |>
    collect()|>
    select(started_at)|>
    filter(!is.na(started_at))
```

```{r}

dataset_rideTypes |>
    
    group_by(rideable_type) |>
    write_dataset(path = "tripdata/ride_types", 
                  format = "parquet")
```

```{r}

tibble (files = list.files ( parquetPath,recursive = TRUE),
        size_MB = file.size ( file.path ( parquetPath, files ) ) / 1024^2 )
```

```{r}

tibble ( files = list.files ( "tripdata", recursive = FALSE),
         size_MB = file.size ( 
             file.path ( "tripdata", files ) ) / 1024^2
         )
```

```{r}
sum(file.size(file.path("tripdata", list.files("tripdata")))/1024^2)
```

There is a significant size difference between the parquet file and the csv files.

Trying the duckdb approach

```{r}
library(duckdb)

dbcon <- dbConnect(duckdb::duckdb())

rideType_pq |>
    
    to_duckdb()

```

```{r}

query <- dataset_rideTypes |>
    
    filter(Ride)
```

```{r}

# list.files {base}
csvFileList <- list.files ( path = "tripdata", 
                            full.names = TRUE )
```

View list.

```{r}

# view {tibble} 
print ( csvFileList )
```

### Data Mapping

```{r}

# map {purrr}, 

# With list_rbind(), map can be used to directly create a dataframe from a list of files

oneTibble <- map ( csvFileList[1:2], read_csv ) |>
    
    list_rbind  ()
```

( In R a tibble is synonymous with a data frame )

```{r}

typeof (oneTibble)
```

```{r}

is_tibble ( oneTibble )
```

```{r}
is.data.frame ( oneTibble )
```

```{r}
oneTibble[1,]
```

```{r}
length(oneTibble[[1]])
```

```{r}

# drop_na {tidyr}	
oneTibble <- drop_na ( oneTibble )
```

```{r}
tibble ( Complete_Observations = 
             length ( oneTibble [[1]] ) )
```

```{r}

# Should only need to check ride_id then datetime columns for duplicates

# Maybe only need to check ride_id column? Actually, no.

# pick {dplyr}	
distinct ( oneTibble, pick ( "ride_id" ) ) |>
    
    # count {dplyr}	
    count ( name = "Distinct Ride ID's" )
```

```{r}

length ( oneTibble [[1]] )
```

```{r}

# This is a separate table used to analyze the observations returned as not distinct (n > 1). This adds an extra column (n) so didn't want to mess with the main dataframe for this.

dupTibble <- oneTibble |>
    
    # adds a count of rows for column 'n'
    add_count ( started_at, 
                ended_at, 
                start_station_name, 
                end_station_name ) |>
    
    # so that only observations that have been duplicated 1 or more         times are shown
    filter ( n > 1 ) |>
    
    # will have to retest if I need this function
    distinct ( ) |>
    
    # because we want to see all the rows, not just one row for each obs
    ungroup ( ) |>
    
    arrange ( desc ( started_at ) )
```

```{r}

print ( dupTibble[3:4] )
```

```{r}

# verify how many rows have/are duplicates
 
# nrow {base}
nrow ( dupTibble )
```

```{r}

# issue is, we need to get rid of not all of these rows, but just the extra duplicate observations. 

# If there were 2 rows of duplicates, we would want to end up with 1 row after removing the extras.

undupdTibble <- dupTibble |>
    
    # distinct {dplyr}
    distinct ( started_at, 
               start_station_name, 
               ended_at, 
               end_station_name, 
               .keep_all = TRUE )
```

```{r}

print ( undupdTibble[3:4] )
```

```{r}

nrow ( undupdTibble )
```

```{r}

distinct ( oneTibble, pick ( "ride_id" ) ) |>
    
    # count {dplyr}	
    count ( name = "Uncorrected Distinct Observations" )
```

```{r}

distinct ( oneTibble, pick ( "started_at",
                             "start_station_name",
                             "ended_at", 
                             "end_station_name" ) ) |>
    # count {dplyr}	
    count ( name = "Corrected Distinct Observations" )
```

```{r}

oneTibble <- oneTibble |>
    
    distinct ( started_at, 
               start_station_name, 
               ended_at, 
               end_station_name, 
               .keep_all = TRUE )
```

```{r}
count ( oneTibble,
        name =
            "Total Observations: Un-duplicated Dataset" )
```

```{r}
# sorting the entire table by the start_at column
oneTibble[3] |>
    
    # arrange {dplyr}	
    arrange ( started_at ) |>
    
    head ()
```

```{r}

oneTibble <- oneTibble |>

    # arrange {dplyr}
    arrange ( started_at )
```

```{r}
print(head(oneTibble[3:4]))
```

```{r}

# Split cells
oneTibble[3] |>
    
    # separate_wider_delim {tidyr}
    separate_wider_delim ( cols = c(started_at),
                           delim = " ",
                           names = c("start_date", 
                                    "start_time"),
                           cols_remove = TRUE) |>
    head()
```

```{r}
# Split cells
oneTibble[3] |>
    
    # separate_wider_delim {tidyr}
    separate_wider_delim ( cols = c(started_at),
                           delim = " ",
                           names = c("start_date", 
                                    "start_time"),
                           cols_remove = TRUE ) |>
    head ()
```

```{r}

oneTibble[4] |>
    
    separate_wider_delim ( cols = c(ended_at),
                           delim = " ",
                           names = c("end_date", 
                                    "end_time"),
                           cols_remove = TRUE ) |>
    
    head()
```

```{r}
# Split cells
oneTibble <- oneTibble |>
    
    # separate_wider_delim {tidyr}
    separate_wider_delim ( cols = c ( started_at ),
                           delim = " ",
                           names = c ( "start_date",
                                       "start_time" ),
                           cols_remove = TRUE )
```

```{r}
tibble_row ( "How many columns?" = ncol ( oneTibble ) )
```

```{r}

oneTibble <- oneTibble |>
    
    separate_wider_delim ( cols = c(ended_at),
                           delim = " ",
                           names = c("end_date", 
                                    "end_time"),
                           cols_remove = TRUE )
```

```{r}
tibble_row ( "How many columns?" = ncol ( oneTibble ) )
```

Â 

```{r}

head ( oneTibble )
```

```{r}

oneTibble2 <- oneTibble |>
    
    group_by ( rideable_type ) |>
    
    # nest {tidyr}
    nest ()
```
