---
title: "Professional Data Analysis"
author: "Eric Mossotti"

code-fold: true

#df_print: "tibble" 

format: html
toc: true
---

#### Future Plans:

-   [Use Quarto Pub for project deployment](https://quarto.org/docs/publishing/quarto-pub.html) ?
-   [GitHub.io](github.io)
-   JS - Observable
    -   Arquero
    -   Observable Plot - [Mermaid \| Diagramming and charting tool](https://mermaid.js.org/) -

#### References:

-   [Why DuckDB?](https://duckdb.org/why_duckdb)

-   [R for Data Science: Chapter 22: Arrow](https://r4ds.hadley.nz/arrow "Arrow")

-   <https://en.wikipedia.org/wiki/Great-circle_distance>

```{r include = FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)
```

### Overview

The dataset is immense with millions of observations from just a single year. It seems to be regularly updated with more data, interestingly enough, despite being simulated.

This doesn't require much domain knowledge to understand, but it is realistic (with errors) and challenging enough for data analysis.

## Data Import and Cleaning

.ZIP files are commonplace as they reduce long term storage requirements. There may already be a function that does this but it seemed a custom function was needed to unzip and relocate .CSV's to a separate folder. The .ZIP files folder deleted from the working directory after relocating files to save on overhead.

```{r}

# Lists of download / zips / file addresses. 

durls <-
    sprintf("https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip",
            202301:202312)

tempZipPaths <- sprintf("tempZips/%d-divvy-tripdata.zip",
                     202301:202312)

fileNames <- sprintf("%d-divvy-tripdata.csv",
                      202301:202312)

tempfile_paths <- sprintf("tempFiles/%d-divvy-tripdata.csv",
                      202301:202312)

fileList <- sprintf("tripdata/%d-divvy-tripdata.csv",
                       202301:202312)

# Need some directories to store the files. 
dir.create("tempZips")

dir.create("tripdata")

dir.create("tempFiles")

# review address info that was just created
tibble::tibble("URLs" = durls,
               "Zip File Paths" = tempZipPaths,
               "File Names" = fileNames,
               "Parquet File Paths" = tempfile_paths)
```

```{r}

# A simple way to download and relocate several files. 
curl::multi_download(durls,
                     destfiles = tempZipPaths)
```

Converting the CSV files to Parquet saves on overhead, also. With CSV files, the data files in their original form were \~1GB; with Parquet, around 0.3GB. It seemed overkill to have a partitioned dataset filesystem for only 0.3GB of data.

```{r}

# A custom function that makes unzipping, converting and relocating files all at once, simple. 
unz_relocate <- function (x = tempfile_paths,
                          y = tempZipPaths,
                          z = fileNames) {
    for (i in seq(x)) {
        utils::unzip(y[i],
                     z[i])
        file.rename(z[i],
                    x[i])
    }
}

unz_relocate()

unlink("tempZips",
       recursive = TRUE)

tripTibble <- purrr::map(tempfile_paths[1:12],
                         arrow::read_csv_arrow) |>
    purrr::list_rbind()

original_nobs <- nrow(tripTibble)

tripTibble <- tripTibble |>
    tidyr::drop_na()
```

```{r}

tripTibble |>
    dplyr::mutate(
        "trip_time" =
            lubridate::time_length(lubridate::interval(started_at,
                                                       ended_at),
                                   unit = "minute"),
        .keep = "all"
    ) |>
    arrow::write_dataset("tempFiles",
                         existing_data_behavior = "delete")

rm(tripTibble)

fileList <- list.files(path = "tempFiles",
                              full.names = TRUE,
                              recursive = TRUE)

cat("Files now in tempFiles: ",
    "\n\n",
    fileList)
```

```{r}

tripset <- arrow::open_dataset(sources = fileList[1],
                               format = "parquet") |>
    dplyr::group_by(month = lubridate::month(started_at),
                    weekday = lubridate::wday(started_at),
                    hour = lubridate::hour(started_at),
                    minute = lubridate::minute(started_at)) |>
    dplyr::select(
        ride_id,
        rideable_type,
        started_at,
        start_station_name,
        start_station_id,
        ended_at,
        end_station_name,
        end_station_id,
        start_lat,
        start_lng,
        end_lat,
        end_lng,
        member_casual,
        trip_time,
        month,
        weekday,
        hour,
        minute
    ) |>
   dplyr::ungroup()


tripset |>
    arrow::to_duckdb() |>
    dplyr::collect() |>
    colnames() |>
    as.matrix()

tripset |> arrow::write_dataset(path = "tripdata",
                                format = "parquet",
                                existing_data_behavior = "delete")

unlink("tempFiles", recursive = TRUE)

tripset <- arrow::open_dataset(sources = "tripdata",
                              format = "parquet")

tripset |>
    arrow::to_duckdb() |>
    dplyr::collect() |>
    colnames() |>
    as.matrix()
# To verify the location of the extracted data.
fileList <- list.files(path = "tripdata",
                              full.names = TRUE,
                              recursive = TRUE)

cat("Files in ~/tripdata: ",
    "\n\n",
    fileList)
```

```{r}

# This is a separate table used to analyze the observations returned as not distinct (n > 1). This adds an extra column labeled "n".
dupeTable <- tripset |>
    arrow::to_duckdb() |>
    dplyr::select(started_at:end_station_name) |>
    # Counts of unique rows added for column 'n'
    dplyr::add_count(started_at,
                     ended_at,
                     start_station_name,
                     end_station_name) |>
    # Only observations that have been duplicated 1 or more 
    # times are shown
    dplyr::filter(n > 1) |>
    # We want to see all rows, not just one row for each obs
    dplyr::ungroup() |>
    dplyr::arrange(started_at) |>
    arrow::to_arrow() |>
    dplyr::collect()
```

We started with 5,719,877 observations (obs) for dates spanning January to December, 2023, then removed 1,388,170 incomplete obs.

Of the other columns, it seems that the start_time, end_time, start_station, and end_station could show if there are possibly hidden duplicated observations. Those 4 variables combined results in the most granular view of any one observation. Meaning, that data would naturally only have duplicates in error.

I assumed that having the exact same times/dates and stations for two different ride IDs is a mistake. Although, I do not know how that error would happen. I could have assumed one person could check out multiple bikes at once. In that instance, each bike could be assigned a unique ride_id. That, however, has only happened 18 times over a year. Since it's only one copy every time, that also raises a red flag. I did not notice any other correlations with station_id/name, member_casual, or ride_type for those particular duplicated data.

```{r}

n <- dupeTable |> 
    dplyr::distinct(n) |>
    as.integer()

cat(" Distinct copy count of dupes: ", n,
    "\n\n",
    "Total observations that have and are duplicates: ",
       length(dupeTable[[1]]))
```

By applying distinct() on dupeTable, we see the only distinct value is 2. We can safely conclude that, of the duplicates, each has a minimum and maximum of 1 extra copy.

Number of rows in the dupeTable is 36. Because each duplicated observation has one duplicate (n = 2), expected removed nobs is 18.

```{r}

# The issue is, we need to get rid of not all of these rows, but just the extra duplicate observations. 

# If there were 2 rows of duplicates, we would want to end up with 1 row after removing the extras.
undupedTable <- dupeTable |>
    arrow::to_duckdb() |>
    dplyr::distinct(started_at,
                     start_station_name,
                     ended_at,
                     end_station_name,
                     .keep_all = TRUE) |>
    arrow::to_arrow() |>
    dplyr::collect()

n <- undupedTable |>
    dplyr::select(started_at) |>
    dplyr::distinct() |>
    dplyr::count() |>
    as.integer()

cat("Count of distinct observations: ", n)
```

The count of observed distinct values for the un-duplicated table was indeed 18.

```{r}

# Run a count on how many rows or observations there are in the dataset.
incorrectDistinct <- tripset |>
    arrow::to_duckdb() |>
    dplyr::distinct(dplyr::pick("ride_id")) |>
    dplyr::count(name = "Incorrect Distinct Observations") |>
    arrow::to_arrow() |>
    dplyr::collect() |>
    as.integer()

correctDistinct <- tripset |>
    arrow::to_duckdb() |>
    dplyr::distinct(
        dplyr::pick(
            "started_at",
            "start_station_name",
            "ended_at",
            "end_station_name"
        )
    ) |>
    dplyr::count() |>
    arrow::to_arrow() |>
    dplyr::collect() |>
    as.integer()

correctionTibble <-
    tibble::tibble(
        "Original Obs" = original_nobs,
        "Uncorrected Complete Obs" = incorrectDistinct,
        "Corrected Complete Obs" = correctDistinct,
        "Removed Obs" = (incorrectDistinct - correctDistinct)
    )

correctionTibble
```

The incorrect number of observations (nobs) was 4,331,707. The correct nobs after removing duplicated obs was 4,331,689. In short, 18 additional obs were removed.

```{r}

tripset |>
    dplyr::select(ride_id:minute) |>
    arrow::to_duckdb() |>
    dplyr::distinct(started_at, 
                    start_station_name, 
                    ended_at,
                    end_station_name, 
                    .keep_all = TRUE) |>
    dplyr::arrange(started_at) |>
    arrow::to_arrow() |>
    dplyr::collect() |>
    arrow::write_dataset(
        path = "tripdata",
        format = "parquet",
        existing_data_behavior = "overwrite"
    )
```

Noting that it was useful for me to retain certain rows at first to determine if there were duplicates.

```{r}

tripset <- arrow::open_dataset(sources = "tripdata",
                               format = "parquet")

tripset |>
    arrow::to_duckdb() |>
    dplyr::filter(trip_time > 1,
                  trip_time < 480) |>
    dplyr::arrange(trip_time) |>
    dplyr::collect() |>
    dplyr::mutate(
        miles = geosphere::distGeo(
            p1 = cbind(start_lng, start_lat),
            p2 = cbind(end_lng, end_lat)
        ) / 1000 * 0.62137119,
        mph = (miles / (trip_time / 60))
    ) |>
    dplyr::filter(miles > 0.1,
                  mph < 16) |>
    dplyr::select(trip_time,
                  miles,
                  mph) |>
    summary()
```

# TESTING SECTION

```{r}

tripset <- arrow::open_dataset(sources = "tripdata",
                               format = "parquet")

dbconn <- DBI::dbConnect(duckdb::duckdb())

# For querying the arrow dataset with the benefits of an OLAP database. 
duckdb::duckdb_register_arrow(dbconn,
                              "trip_data",
                              tripset)


dplyr::tbl(dbconn,
           "trip_data") |>
    colnames() |>
    as.matrix()
```

```{r}

decimalTibble <- tripset |>
    dplyr::select(end_station_id) |>
    arrow::to_duckdb() |>
        dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id, "\\.[:digit:]$")) |>
    dplyr::count() |>
    arrow::to_arrow() |>
    dplyr::collect()

check1079 <- tripset |>
    dplyr::select(end_station_id) |>
    arrow::to_duckdb() |>
    dplyr::group_by(end_station_id) |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "[1][0][7][9]")) |>
    dplyr::count() |>
    arrow::to_arrow() |>
    dplyr::collect()

doubleDecimalTibble <- tripset |>
    dplyr::select(end_station_id) |>
    arrow::to_duckdb() |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[1-9]$")) |>
    dplyr::arrange(end_station_id) |>
    arrow::to_arrow() |>
    dplyr::collect()

removes_oneDecimalNum <- tripset |>
    dplyr::select(end_station_id) |>
    arrow::to_duckdb() |>
    dplyr::filter(stringr::str_detect(end_station_id,
                                      "\\.[:digit:]$")) |>
    dplyr::mutate(
        end_station_id = stringr::str_replace(
            string = end_station_id,
            pattern = "\\.[:digit:]$",
            replacement = "")
        ) |>
    arrow::to_arrow() |>
    dplyr::collect()
```

It doesn't really seem like the station IDs containing decimals (e.g., ####.# or ####.#.#) were typos. There were no station ids with decimal numbers other than 0 or 1 after the decimal place. There were many different station IDs with one 0 after the decimal. There was only 1 station ID with a 1 following the decimal and it was in the form of "###.1.1".

```{sql, connection = dbconn, output.var = "riders_monthly"}

SELECT 
    month,
    COUNT(member_casual) AS rider_count 
FROM 
    trip_data 
GROUP BY 
    month 
ORDER BY 
    month
```

```{r}

# Making the data a little more visual.
riders_monthly <- riders_monthly |>
    dplyr::mutate(month = month.abb) |>
    dplyr::arrange(rider_count) |>
    dplyr::rename("Month" = month,
                  "Riders" = rider_count) 

ggplot2::ggplot(riders_monthly,
                     ggplot2::aes(x = reorder(Month, Riders),
                                  y = Riders,
                                  fill = Riders)) +
    ggplot2::geom_col(color = 'black') +
    ggplot2::scale_y_continuous(
        labels = scales::label_comma(big.mark = ",")) +
    ggplot2::scale_fill_viridis_b() +
    ggplot2::labs(
        title = "Rider Counts - Monthly",
        subtitle = "(2023)",
        caption = "Data retrieved from Cyclistic database.",
        tag = "Figure 1.b",
        x = "Month (Jan - Dec)",
        y = "Riders"
    )
```

```{r}

sqlText <- paste0(
    "SELECT 
        rideable_type, count() AS 'rt_counts' ",
    "FROM 
        trip_data ",
    "GROUP BY 
        rideable_type ",
    "ORDER BY 
        rt_counts"
)

queryResult <- duckdb::dbSendQuery(conn = dbconn,
                                   statement = sqlText)

fetchedRes <- duckdb::dbFetch(queryResult)

ggplot2::ggplot(data = fetchedRes) +
    ggplot2::geom_col(mapping = ggplot2::aes(
        x = reorder(rideable_type,
                    rt_counts),
        y = rt_counts,
        fill = rt_counts
    )) +
    ggplot2::scale_y_continuous(
        labels = scales::label_comma(big.mark = ",")) +
    ggplot2::scale_fill_distiller(palette = "Spectral",
                                  direction = -1) +
    ggplot2::labs(
        title = "Rental Count - by Type",
        subtitle = "(Jan-Dec, 2023)",
        caption = "Data from cyclistic database.",
        tag = "Figure 1.a",
        x = "Ride Types",
        y = "Count"
    )
```

```{r}

dplyr::tbl(dbconn,
           "trip_data") |>
    colnames() |>
    as.matrix()
```

## Smart Filtering the Database

Ensuring we only are making conclusions about sensible data by first filtering out the outliers.

```{r}

dplyr::tbl(dbconn,
           "trip_data") |>
    dplyr::filter(trip_time > 1,
                  trip_time < 480) |>
    dplyr::collect() |>
    dplyr::mutate(
        miles = geosphere::distGeo(
            p1 = cbind(start_lng, start_lat),
            p2 = cbind(end_lng, end_lat)
        ) / 1000 * 0.62137119,
        mph = (miles / (trip_time / 60))
    ) |>
    dplyr::filter(miles > 0.1,
                  mph < 16,
                  mph > 1) |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "fltrd_TripData")

# to see how many observations are left
dplyr::tbl(dbconn, "fltrd_TripData") |>
    dplyr::select(ride_id) |>
    dplyr::collect() |>
    nrow()
```

## TIME TABLE - COMPREHENSIVE

```{r}

##### more comprehensive, less confusing, and potentially saving a lot of time and code

dplyr::tbl(dbconn,
           "fltrd_TripData") |>
    dplyr::select(month,
                  weekday,
                  hour,
                  minute,
                  rideable_type,
                  member_casual,
                  trip_time,
                  miles,
                  mph
                  ) |>
    dplyr::group_by(month,
                    weekday,
                    hour,
                    minute,
                    rideable_type,
                    member_casual) |> 
    dplyr::summarize("rider_count" = dplyr::count(member_casual),
                     "avg_tripTime" = mean(trip_time),
                     "avg_miles" = mean(miles),
                     "avg_mph" = mean(mph)) |>
    dplyr::arrange(month,
                   weekday,
                   hour,
                   minute) |>
    dplyr::collect() |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "TimeTable")

dplyr::tbl(dbconn, 
           "TimeTable") |>
    head()
```

```{r, eval = FALSE}

dplyr::tbl(dbconn,
           "") |>
    dplyr::left_join(y = dplyr::tbl(dbconn,
                                    "")) |>
    dplyr::collect() |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "")

dplyr::tbl(dbconn, "")
duckdb::dbListTables(dbconn)
```

end_station_name2member\
end_station_name2type

```{r, eval = FALSE}

# a chunk for easily dropping either all (default) or specific tables from the ddb 

paths <- duckdb::dbListTables(dbconn)

paths[]

drops_tables <- function(path) {
    
    duckdb::dbRemoveTable(dbconn, 
                          path)
}

paths[2] |> purrr::walk(drops_tables)

paths <- duckdb::dbListTables(dbconn)

paths[]
```

```{r, eval = FALSE}

dplyr::tbl(dbconn,
           "") |>
    dplyr::left_join(y = dplyr::tbl(dbconn,
                                    "")) |>
    dplyr::left_join(y = dplyr::tbl(dbconn,
                                    "")) |>
    dplyr::left_join(y = dplyr::tbl(dbconn,
                                    "")) |>
    dplyr::left_join(y = dplyr::tbl(dbconn,
                                    ""))
```

## Statistics

We don't need to apply statistical inference techniques such as random sampling since we already have the complete population data.

Exploratory data analysis:

-   Counts of grouped data

    -   Dodged bar chart (member vs casual?)
    -   One graph for each variable-group, faceted

-   Median

    -   Comparing medians between groups (trip_time, hour, weekday?)

        -   Histogram

            -   data densities

                -   area plots

    -   Facet graphs

    -   Scatter-plot with line of fit

        -   determine if trends are linear or nonlinear

-   Modality

    -   Is it uni, bi, or multi?

Midpoint formula variation for finding the midpoint between all pairs of coords. ??

distm()

map_data()

pi + coord_quickmap()

pi + coord_map()

```{mermaid}
erDiagram
```

[Graph/Network Visualization • DiagrammeR (rich-iannone.github.io)](https://rich-iannone.github.io/DiagrammeR/)
